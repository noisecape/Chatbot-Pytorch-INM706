{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from dataset import CornellCorpus\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Encoder, Decoder, ChatbotModel, EncoderAttention, LuongAttentionDecoder, Attention\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check cuda availability\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the dir where to save the trained model\n",
    "save_model_dir = 'saved_models'\n",
    "if not os.path.exists(save_model_dir):\n",
    "    os.mkdir(save_model_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "\n",
    "def get_movie_lines(path):\n",
    "    \"\"\"\n",
    "    This function extracts the movie lines id and the text associated\n",
    "    and store them in a dictionary.\n",
    "    :param path: the path where to find the file 'movie_lines.txt'\n",
    "    :return line_to_phrase: the dictionary that maps each line id to the corresponding text\n",
    "    \"\"\"\n",
    "    file = open(path, 'r', encoding='iso-8859-1')\n",
    "    dialog_data = []\n",
    "    line_to_phrase = {}\n",
    "    for line in file.readlines():\n",
    "        dialog_data.append(line.strip().split(sep=' +++$+++ '))\n",
    "    for information in dialog_data:\n",
    "        line_to_phrase[information[0]] = information[-1].replace('\\n', '')\n",
    "    return line_to_phrase\n",
    "\n",
    "\n",
    "def extract_dialogs():\n",
    "    \"\"\"\n",
    "    This function extracts dialogs from each movie. A dialog is represented by\n",
    "    a list on lineid which identifies a unique conversation in the dataset.\n",
    "    :return conversation:\n",
    "    \"\"\"\n",
    "    PATH_CONVERSATION = os.path.join(os.curdir, 'cornell-movie-dialogs-corpus/movie_conversations.txt')\n",
    "    file = open(PATH_CONVERSATION, 'r', encoding='iso-8859-1')\n",
    "    dialog_list = []\n",
    "\n",
    "    # extract conversations info from 'movie_conversation.txt'\n",
    "    for line in file.readlines():\n",
    "        line = line.split(' +++$+++ ')\n",
    "        regex = re.compile('[^a-zA-Z0-9,]')\n",
    "        line = regex.sub('', line[-1])\n",
    "        line = line.split(',')\n",
    "        dialog_list.append(line)\n",
    "\n",
    "    return dialog_list\n",
    "\n",
    "\n",
    "def create_pair_dialogs(dialogs):\n",
    "    # dictionary that stores the following [question] -> [answer] for each line in a dialog\n",
    "    dialogs_pairs = []\n",
    "    for dialog in dialogs:\n",
    "        question_to_answer = {}\n",
    "        for id in range(len(dialog) - 1):\n",
    "            question_line = dialog[id]\n",
    "            answer_line = dialog[id+1]\n",
    "            # check if either the answer or the question is empty and if that's the case don't append it.\n",
    "            if question_line and answer_line:\n",
    "                question_to_answer[question_line] = answer_line\n",
    "                dialogs_pairs.append(question_to_answer)\n",
    "    return dialogs_pairs\n",
    "\n",
    "\n",
    "def save_models(model):\n",
    "    PATH = save_model_dir + '/chatbot_model'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def load_model(model):\n",
    "    PATH = save_model_dir + '/chatbot_model'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "def format_time(start, end):\n",
    "    elapsed_time = end - start\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_secs, elapsed_mins"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the Vocabulary class\n",
    "\n",
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, idx_to_text, dialogs_ids):\n",
    "        self.dialogs_ids = dialogs_ids\n",
    "        self.idx_to_text = self.normalize_sentence(idx_to_text)\n",
    "        self.word_to_idx = self.map_word_to_idx()\n",
    "        self.vocab = self.map_idx_to_word()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "    def normalize_sentence(self, idx_to_text):\n",
    "        normalized_idx_to_sentence = {}\n",
    "        for line_id, sentence in zip(idx_to_text.keys(), idx_to_text.values()):\n",
    "            # convert each word in the sentence to a lower case\n",
    "            sentence = sentence.lower()\n",
    "            # eliminate extra spaces for punctuation\n",
    "            sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "            # remove non-letter characters but keep regular punctuation\n",
    "            sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "            normalized_idx_to_sentence[line_id] = sentence\n",
    "        return normalized_idx_to_sentence\n",
    "\n",
    "    def map_idx_to_word(self):\n",
    "        words = self.word_to_idx.keys()\n",
    "        index = self.word_to_idx.values()\n",
    "        idx_to_word = OrderedDict()\n",
    "        for w, i in zip(words, index):\n",
    "            idx_to_word[i] = w\n",
    "        return idx_to_word\n",
    "\n",
    "    def map_word_to_idx(self):\n",
    "        word_to_idx = OrderedDict()\n",
    "        count_words = 0\n",
    "        pad_token = '<PAD>'\n",
    "        word_to_idx[pad_token] = count_words\n",
    "        count_words += 1\n",
    "        for dialogs in self.dialogs_ids:\n",
    "            for line in dialogs:\n",
    "                sentence = self.idx_to_text[line]\n",
    "                for word in sentence.split():\n",
    "                    if word not in word_to_idx:\n",
    "                        word_to_idx[word] = count_words\n",
    "                        count_words += 1\n",
    "        start_token = '<S>'\n",
    "        word_to_idx[start_token] = count_words\n",
    "        count_words += 1\n",
    "        end_token = '</S>'\n",
    "        word_to_idx[end_token] = count_words\n",
    "        count_words += 1\n",
    "        unknown_token = '<UNK>'\n",
    "        word_to_idx[unknown_token] = count_words\n",
    "        return word_to_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define all the parameters for the model\n",
    "\n",
    "dir = os.path.join(os.curdir, 'cornell-movie-dialogs-corpus/movie_lines.txt')\n",
    "idx_to_text = get_movie_lines(dir)\n",
    "# create a list of dialogs for each movie.\n",
    "dialogs = extract_dialogs()\n",
    "# for each movie, create pairs dialogs (Q/A). This is the actual data used for training.\n",
    "pair_dialogs_idx = create_pair_dialogs(dialogs)\n",
    "# instantiate the vocabulary\n",
    "vocabulary = Vocabulary(idx_to_text, dialogs)\n",
    "print('Total words counted in the vocabulary: {}'.format(vocabulary.__len__()))\n",
    "# create the dataset class to store the data\n",
    "train_data = CornellCorpus(pair_dialogs_idx, vocabulary, train_data=True)\n",
    "val_data = CornellCorpus(pair_dialogs_idx, vocabulary, train_data=False)\n",
    "# hyperparameters\n",
    "batch_size = 128\n",
    "hidden_size = 256\n",
    "embedding_size = 256\n",
    "epochs = 2000\n",
    "optim_parameters = {'lr': 1e-4, 'weight_decay': 1e-3}\n",
    "\n",
    "# init dataloader\n",
    "load_args = {'batch_size': batch_size, 'shuffle': True}\n",
    "train_dataloader = DataLoader(train_data, **load_args)\n",
    "val_dataloader = DataLoader(val_data, **load_args)\n",
    "print(train_dataloader.__len__())\n",
    "print(val_dataloader.__len__())\n",
    "# init seq2seq model, the parameters needed are\n",
    "# embedding_size -> the size of the embedding for each word\n",
    "# hidden_size -> the number of hidden neurons per unit\n",
    "# voc_size -> the size of the vocabulary to embed each word\n",
    "# encoder = Encoder(embedding_size, hidden_size, vocabulary.__len__()).to(device)\n",
    "# decoder = Decoder(embedding_size, hidden_size, vocabulary.__len__()).to(device)\n",
    "\n",
    "encoder = EncoderAttention(embedding_size, hidden_size, vocabulary.__len__()).to(device)\n",
    "attention = Attention(hidden_size).to(device)\n",
    "decoder = LuongAttentionDecoder(embedding_size, hidden_size, vocabulary.__len__(), attention=attention).to(device)\n",
    "\n",
    "model = ChatbotModel(encoder, decoder, vocabulary.__len__(), attention=True).to(device)\n",
    "#init the optimizer\n",
    "optim = optim.Adam(model.parameters(), **optim_parameters)\n",
    "# init loss function\n",
    "# when computing the loss you want to ignore the '<PAD>' token.\n",
    "pad_index = vocabulary.word_to_idx['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index).to(device)\n",
    "epoch_history = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    batch_history = []\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    for id, X in enumerate(train_dataloader):\n",
    "        # transpose both input sentence and target sentence to access using the first dimension\n",
    "        # the the i-th word for each batch at each given time step t.\n",
    "        question = torch.transpose(X[0], 0, 1)\n",
    "        answer = torch.transpose(X[1], 0, 1)\n",
    "        # compute the output. Recall the output size should be (seq_len, batch_size, voc_size)\n",
    "        output = model(question, answer)\n",
    "        # don't consider the first element in all batches because it's the '<S>' token\n",
    "        output = output[1:]\n",
    "        answer = answer[1:]\n",
    "        # reshape both question and answer to the correct size for the loss function\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        answer = answer.reshape(-1)\n",
    "        # default previous weights values\n",
    "        optim.zero_grad()\n",
    "        # compute loss to backpropagate\n",
    "        loss = criterion(output, answer)\n",
    "        # backpropagate to compute gradients\n",
    "        loss.backward()\n",
    "        # clip gradients to avoid exploding values\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optim.step()\n",
    "        batch_history.append(loss.item())\n",
    "        # print current loss every 500 processed batches\n",
    "        if id % 500 == 0:\n",
    "            print('BATCH [{}/{}], LOSS: {}'.format(id, train_dataloader.__len__(), loss))\n",
    "    avg_loss = np.sum(batch_history) / train_dataloader.__len__()\n",
    "    return avg_loss\n",
    "\n",
    "def val_loop():\n",
    "    model.eval()\n",
    "    batch_history = []\n",
    "    avg_loss = 0\n",
    "    for idx, X in enumerate(val_dataloader):\n",
    "        # transpose both input sentence and target sentence to access using the first dimension\n",
    "        # the the i-th word for each batch at each given time step t.\n",
    "        question = torch.transpose(X[0], 0, 1)\n",
    "        answer = torch.transpose(X[1], 0, 1)\n",
    "        # compute the output. Recall the output size should be (seq_len, batch_size, voc_size)\n",
    "        output = model(question, answer)\n",
    "        # don't consider the first element in all batches because it's the '<S>' token\n",
    "        output = output[1:]\n",
    "        answer = answer[1:]\n",
    "        # reshape both question and answer to the correct size for the loss function\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        answer = answer.reshape(-1)\n",
    "        # compute the loss\n",
    "        loss = criterion(output, answer)\n",
    "        # keep track of the loss\n",
    "        batch_history.append(loss.item())\n",
    "        # print current loss every 500 processed batches\n",
    "        if id % 500 == 0:\n",
    "            print('BATCH [{}/{}], LOSS: {}'.format(id, val_dataloader.__len__(), loss))\n",
    "    avg_loss = torch.sum(batch_history)/ val_dataloader.__len__()\n",
    "    return avg_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Start training...')\n",
    "### TRAIN LOOP ###\n",
    "for epoch in range(epochs):\n",
    "    # start counting epoch time\n",
    "    start_time = time.time()\n",
    "    # compute train loss\n",
    "    train_loss = train_loop()\n",
    "    # compute val loss\n",
    "    val_loss = val_loop()\n",
    "    # store train and val loss for later analysis\n",
    "    epoch_history.append((train_loss, val_loss))\n",
    "    # end of epoch\n",
    "    end_time = time.time()\n",
    "    # format elapsed time\n",
    "    elapsed_secs, elapsed_mins = format_time(start_time, end_time)\n",
    "    print(\"EPOCH [{}/{}] | Train Loss: {} | Val. Loss: {} | time: {}m {} s\".format(epoch+1,\n",
    "                                                                                       epochs,\n",
    "                                                                                       train_loss,\n",
    "                                                                                       val_loss,\n",
    "                                                                                       elapsed_mins,\n",
    "                                                                                       elapsed_secs))\n",
    "# save training model.\n",
    "print('Training completed.')\n",
    "save_models(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}