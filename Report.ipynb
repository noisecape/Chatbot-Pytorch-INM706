{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from dataset import CornellCorpus\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Encoder, Decoder, ChatbotModel, EncoderAttention, LuongAttentionDecoder, Attention, GreedySearch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check cuda availability\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(device)\n",
    "\n",
    "# define the dir where to save the trained model\n",
    "save_model_dir = 'saved_models'\n",
    "if not os.path.exists(save_model_dir):\n",
    "    os.mkdir(save_model_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "\n",
    "def get_movie_lines(path):\n",
    "    \"\"\"\n",
    "    This function extracts the movie lines id and the text associated\n",
    "    and store them in a dictionary.\n",
    "    :param path: the path where to find the file 'movie_lines.txt'\n",
    "    :return line_to_phrase: the dictionary that maps each line id to the corresponding text\n",
    "    \"\"\"\n",
    "    file = open(path, 'r', encoding='iso-8859-1')\n",
    "    dialog_data = []\n",
    "    line_to_phrase = {}\n",
    "    for line in file.readlines():\n",
    "        dialog_data.append(line.strip().split(sep=' +++$+++ '))\n",
    "    for information in dialog_data:\n",
    "        line_to_phrase[information[0]] = information[-1].replace('\\n', '')\n",
    "    return line_to_phrase\n",
    "\n",
    "\n",
    "def extract_dialogs():\n",
    "    \"\"\"\n",
    "    This function extracts dialogs from each movie. A dialog is represented by\n",
    "    a list on lineid which identifies a unique conversation in the dataset.\n",
    "    :return conversation:\n",
    "    \"\"\"\n",
    "    PATH_CONVERSATION = os.path.join(os.curdir, 'cornell-movie-dialogs-corpus/movie_conversations.txt')\n",
    "    file = open(PATH_CONVERSATION, 'r', encoding='iso-8859-1')\n",
    "    dialog_list = []\n",
    "\n",
    "    # extract conversations info from 'movie_conversation.txt'\n",
    "    for line in file.readlines():\n",
    "        line = line.split(' +++$+++ ')\n",
    "        regex = re.compile('[^a-zA-Z0-9,]')\n",
    "        line = regex.sub('', line[-1])\n",
    "        line = line.split(',')\n",
    "        dialog_list.append(line)\n",
    "\n",
    "    return dialog_list\n",
    "\n",
    "\n",
    "def create_pair_dialogs(dialogs):\n",
    "    # dictionary that stores the following [question] -> [answer] for each line in a dialog\n",
    "    dialogs_pairs = []\n",
    "    for dialog in dialogs:\n",
    "        question_to_answer = {}\n",
    "        for id in range(len(dialog) - 1):\n",
    "            question_line = dialog[id]\n",
    "            answer_line = dialog[id+1]\n",
    "            # check if either the answer or the question is empty and if that's the case don't append it.\n",
    "            if question_line and answer_line:\n",
    "                question_to_answer[question_line] = answer_line\n",
    "                dialogs_pairs.append(question_to_answer)\n",
    "    return dialogs_pairs\n",
    "\n",
    "def select_n_pairs(data, limit):\n",
    "    \"\"\"\n",
    "    This function limits the number of pairs to use in the dataset\n",
    "    :param data: the list of pairs, of shape [question, answer]\n",
    "    :param limit: the integer that represents the number of pairs to select from the data\n",
    "    :return data: the trimmed data with 'limit' number of pairs.\n",
    "    \"\"\"\n",
    "    return data[:limit]\n",
    "\n",
    "\n",
    "def format_time(start, end):\n",
    "    elapsed_time = end - start\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_secs, elapsed_mins"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define the Vocabulary class\n",
    "\n",
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, idx_to_text, dialogs_ids):\n",
    "        self.dialogs_ids = dialogs_ids\n",
    "        self.idx_to_text = self.normalize_sentence(idx_to_text)\n",
    "        self.word_to_idx = self.map_word_to_idx()\n",
    "        self.vocab = self.map_idx_to_word()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "    def normalize_sentence(self, idx_to_text):\n",
    "        normalized_idx_to_sentence = {}\n",
    "        for line_id, sentence in zip(idx_to_text.keys(), idx_to_text.values()):\n",
    "            # convert each word in the sentence to a lower case\n",
    "            sentence = sentence.lower()\n",
    "            # eliminate extra spaces for punctuation\n",
    "            sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "            # remove non-letter characters but keep regular punctuation\n",
    "            sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "            normalized_idx_to_sentence[line_id] = sentence\n",
    "        return normalized_idx_to_sentence\n",
    "\n",
    "    def map_idx_to_word(self):\n",
    "        words = self.word_to_idx.keys()\n",
    "        index = self.word_to_idx.values()\n",
    "        idx_to_word = OrderedDict()\n",
    "        for w, i in zip(words, index):\n",
    "            idx_to_word[i] = w\n",
    "        return idx_to_word\n",
    "\n",
    "    def map_word_to_idx(self):\n",
    "        word_to_idx = OrderedDict()\n",
    "        count_words = 0\n",
    "        pad_token = '<PAD>'\n",
    "        word_to_idx[pad_token] = count_words\n",
    "        count_words += 1\n",
    "        for dialogs in self.dialogs_ids:\n",
    "            for line in dialogs:\n",
    "                sentence = self.idx_to_text[line]\n",
    "                for word in sentence.split():\n",
    "                    if word not in word_to_idx:\n",
    "                        word_to_idx[word] = count_words\n",
    "                        count_words += 1\n",
    "        start_token = '<S>'\n",
    "        word_to_idx[start_token] = count_words\n",
    "        count_words += 1\n",
    "        end_token = '</S>'\n",
    "        word_to_idx[end_token] = count_words\n",
    "        count_words += 1\n",
    "        unknown_token = '<UNK>'\n",
    "        word_to_idx[unknown_token] = count_words\n",
    "        return word_to_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words counted in the vocabulary: 49561\n",
      "Dataset built\n",
      "Train Dataset dimension: 227\n",
      "Dataset built\n",
      "Validation Dataset dimension: 42\n",
      "OrderedDict([('encoder.embedding.weight', tensor([[-2.0270,  1.1310,  1.9517,  ...,  0.1356,  0.0328, -0.7463],\n",
      "        [ 1.5149, -0.3930,  0.2880,  ...,  2.0454, -0.6105,  0.2214],\n",
      "        [-0.1784, -1.4100,  0.1019,  ...,  0.5301, -1.0997,  1.5477],\n",
      "        ...,\n",
      "        [ 0.7193, -0.0738, -0.9326,  ..., -0.6270,  0.4408, -0.8109],\n",
      "        [ 0.0081, -0.2039,  0.1064,  ..., -1.0171, -1.3911, -1.1890],\n",
      "        [-1.3148, -1.5365, -0.0911,  ...,  1.1403, -1.1790,  0.3046]])), ('encoder.lstm.weight_ih_l0', tensor([[ 0.0419,  0.0762, -0.0454,  ..., -0.0168,  0.0081, -0.0181],\n",
      "        [ 0.0047, -0.0148, -0.0798,  ..., -0.0789, -0.0013, -0.0357],\n",
      "        [-0.0463,  0.0686, -0.0763,  ...,  0.0032,  0.0838, -0.0294],\n",
      "        ...,\n",
      "        [ 0.0409,  0.0320,  0.0630,  ...,  0.0657, -0.0501,  0.0135],\n",
      "        [ 0.0597, -0.0792,  0.0363,  ..., -0.0375,  0.0605,  0.0112],\n",
      "        [ 0.0460,  0.0496,  0.0170,  ...,  0.0466,  0.0176, -0.0382]])), ('encoder.lstm.weight_hh_l0', tensor([[ 0.0837, -0.0086, -0.0418,  ..., -0.0322, -0.0012,  0.0618],\n",
      "        [ 0.0503, -0.0758, -0.0797,  ...,  0.0188, -0.0819, -0.0509],\n",
      "        [-0.0746, -0.0003,  0.0840,  ...,  0.0429, -0.0518, -0.0024],\n",
      "        ...,\n",
      "        [-0.0157, -0.0042,  0.0440,  ...,  0.0690,  0.0062,  0.0162],\n",
      "        [-0.0746,  0.0695, -0.0170,  ...,  0.0262, -0.0653,  0.0528],\n",
      "        [-0.0746,  0.0324, -0.0880,  ...,  0.0577,  0.0065, -0.0378]])), ('encoder.lstm.bias_ih_l0', tensor([-0.0458,  0.0125, -0.0757, -0.0876,  0.0605, -0.0476, -0.0415, -0.0243,\n",
      "        -0.0097, -0.0734,  0.0880, -0.0850,  0.0167,  0.0648,  0.0135,  0.0567,\n",
      "         0.0415, -0.0279, -0.0512,  0.0099, -0.0136,  0.0663, -0.0454,  0.0351,\n",
      "         0.0859, -0.0762,  0.0237,  0.0081, -0.0330, -0.0001, -0.0222,  0.0479,\n",
      "         0.0499,  0.0859, -0.0611, -0.0575,  0.0382, -0.0575, -0.0559,  0.0592,\n",
      "        -0.0090,  0.0235,  0.0574, -0.0055,  0.0657, -0.0653,  0.0668,  0.0237,\n",
      "        -0.0491,  0.0269, -0.0689,  0.0747,  0.0851, -0.0650, -0.0824,  0.0252,\n",
      "         0.0040,  0.0392, -0.0117, -0.0588,  0.0299,  0.0257,  0.0293,  0.0273,\n",
      "         0.0761, -0.0852, -0.0251, -0.0334,  0.0065, -0.0387,  0.0765,  0.0497,\n",
      "         0.0336, -0.0477,  0.0612,  0.0545, -0.0822,  0.0578, -0.0198, -0.0703,\n",
      "        -0.0601, -0.0197,  0.0006, -0.0381,  0.0008,  0.0597, -0.0091, -0.0415,\n",
      "        -0.0016, -0.0479, -0.0501,  0.0575,  0.0715, -0.0402, -0.0654, -0.0069,\n",
      "        -0.0839,  0.0254, -0.0646,  0.0337, -0.0298, -0.0647, -0.0216, -0.0182,\n",
      "        -0.0515, -0.0037, -0.0209, -0.0082, -0.0160,  0.0133,  0.0526,  0.0827,\n",
      "         0.0540,  0.0309,  0.0214, -0.0350, -0.0528, -0.0282,  0.0588,  0.0397,\n",
      "        -0.0752, -0.0081, -0.0840, -0.0358,  0.0225, -0.0510, -0.0744,  0.0747,\n",
      "        -0.0211, -0.0275,  0.0680,  0.0066,  0.0216,  0.0875,  0.0653,  0.0528,\n",
      "         0.0213,  0.0548, -0.0643,  0.0376,  0.0722,  0.0394,  0.0518, -0.0655,\n",
      "         0.0722,  0.0293, -0.0479, -0.0703,  0.0121,  0.0213,  0.0685,  0.0400,\n",
      "        -0.0576, -0.0721,  0.0073,  0.0743, -0.0481,  0.0829, -0.0758, -0.0216,\n",
      "         0.0573,  0.0525,  0.0813,  0.0829, -0.0382,  0.0112, -0.0252,  0.0697,\n",
      "         0.0202, -0.0239,  0.0769, -0.0260,  0.0800,  0.0193, -0.0426,  0.0441,\n",
      "        -0.0188, -0.0450,  0.0515,  0.0281,  0.0052,  0.0099,  0.0294,  0.0232,\n",
      "         0.0517,  0.0194,  0.0529,  0.0331,  0.0537,  0.0530, -0.0281,  0.0726,\n",
      "        -0.0142, -0.0795, -0.0040,  0.0240, -0.0731,  0.0095, -0.0801, -0.0692,\n",
      "         0.0882,  0.0258,  0.0550, -0.0034,  0.0754,  0.0414, -0.0333, -0.0675,\n",
      "        -0.0799, -0.0861, -0.0435, -0.0668, -0.0541, -0.0467,  0.0388,  0.0812,\n",
      "         0.0421,  0.0569,  0.0452,  0.0781, -0.0384, -0.0384,  0.0054,  0.0490,\n",
      "        -0.0230, -0.0115, -0.0148,  0.0529,  0.0007,  0.0335,  0.0009,  0.0648,\n",
      "        -0.0020,  0.0209,  0.0246, -0.0774, -0.0370, -0.0632, -0.0251,  0.0863,\n",
      "        -0.0864, -0.0504, -0.0805,  0.0808,  0.0445,  0.0050, -0.0265,  0.0855,\n",
      "         0.0342, -0.0863,  0.0823, -0.0690, -0.0529, -0.0656, -0.0103,  0.0330,\n",
      "        -0.0767,  0.0294, -0.0475,  0.0586, -0.0237,  0.0786,  0.0805, -0.0359,\n",
      "        -0.0149,  0.0422, -0.0049, -0.0729,  0.0255, -0.0104, -0.0811, -0.0445,\n",
      "        -0.0233,  0.0131, -0.0047,  0.0149, -0.0075, -0.0860, -0.0694,  0.0040,\n",
      "         0.0539, -0.0793,  0.0058, -0.0499, -0.0588, -0.0396, -0.0663, -0.0221,\n",
      "        -0.0492, -0.0102, -0.0617, -0.0283, -0.0068,  0.0307,  0.0043,  0.0410,\n",
      "         0.0131,  0.0427,  0.0509, -0.0079,  0.0446,  0.0162,  0.0175, -0.0242,\n",
      "         0.0251,  0.0678, -0.0442, -0.0779, -0.0290,  0.0405,  0.0092, -0.0489,\n",
      "         0.0366,  0.0702, -0.0031,  0.0639, -0.0267,  0.0625,  0.0530,  0.0471,\n",
      "        -0.0083, -0.0215, -0.0561,  0.0618, -0.0497, -0.0600,  0.0313,  0.0868,\n",
      "         0.0586, -0.0075, -0.0553,  0.0405, -0.0051, -0.0730, -0.0855,  0.0241,\n",
      "        -0.0082,  0.0576, -0.0309, -0.0807, -0.0150, -0.0787, -0.0733, -0.0262,\n",
      "         0.0617,  0.0419,  0.0801,  0.0542, -0.0862, -0.0205,  0.0748, -0.0062,\n",
      "         0.0548, -0.0186,  0.0146, -0.0512, -0.0596, -0.0412, -0.0692,  0.0488,\n",
      "        -0.0685, -0.0103,  0.0677, -0.0046,  0.0061, -0.0544, -0.0116, -0.0878,\n",
      "         0.0703, -0.0345,  0.0112, -0.0517, -0.0824,  0.0064, -0.0682,  0.0518,\n",
      "         0.0018,  0.0081,  0.0158,  0.0172,  0.0588, -0.0881,  0.0792,  0.0811,\n",
      "         0.0565, -0.0256,  0.0557, -0.0178, -0.0712, -0.0080,  0.0402, -0.0595,\n",
      "         0.0743, -0.0277,  0.0115, -0.0009, -0.0370, -0.0720, -0.0836,  0.0013,\n",
      "         0.0869, -0.0012, -0.0297, -0.0530, -0.0396,  0.0112, -0.0312,  0.0321,\n",
      "         0.0264, -0.0362,  0.0353,  0.0763,  0.0879, -0.0807,  0.0272,  0.0450,\n",
      "        -0.0128, -0.0328, -0.0745,  0.0882, -0.0091,  0.0058,  0.0539, -0.0220,\n",
      "         0.0659, -0.0042,  0.0383,  0.0580, -0.0639,  0.0220, -0.0606,  0.0775,\n",
      "        -0.0014, -0.0393, -0.0690, -0.0724, -0.0802, -0.0230,  0.0376,  0.0409,\n",
      "         0.0621, -0.0416,  0.0534, -0.0406,  0.0180, -0.0117,  0.0037, -0.0802,\n",
      "         0.0816, -0.0437,  0.0714,  0.0622, -0.0119, -0.0816,  0.0094, -0.0696,\n",
      "        -0.0755,  0.0571, -0.0877,  0.0131, -0.0652, -0.0308, -0.0743, -0.0548,\n",
      "         0.0065,  0.0580, -0.0083, -0.0721,  0.0210, -0.0382, -0.0483, -0.0091,\n",
      "         0.0054,  0.0197,  0.0292, -0.0560, -0.0334, -0.0716, -0.0587, -0.0877,\n",
      "        -0.0528, -0.0335,  0.0220, -0.0392, -0.0223,  0.0008, -0.0315, -0.0047,\n",
      "         0.0785,  0.0569, -0.0405,  0.0078, -0.0585,  0.0685,  0.0384,  0.0016,\n",
      "         0.0685, -0.0703, -0.0315, -0.0218,  0.0760,  0.0854,  0.0639, -0.0863,\n",
      "        -0.0106, -0.0236, -0.0229,  0.0125, -0.0873,  0.0275,  0.0257, -0.0340])), ('encoder.lstm.bias_hh_l0', tensor([-0.0190, -0.0562,  0.0371, -0.0457, -0.0753, -0.0329,  0.0383,  0.0438,\n",
      "        -0.0823,  0.0437,  0.0789, -0.0344, -0.0487,  0.0001, -0.0433,  0.0711,\n",
      "         0.0738,  0.0570, -0.0712, -0.0239, -0.0321, -0.0247, -0.0077,  0.0109,\n",
      "        -0.0440,  0.0156, -0.0794, -0.0707, -0.0359,  0.0196, -0.0093, -0.0728,\n",
      "        -0.0509,  0.0629,  0.0766,  0.0166,  0.0110,  0.0342,  0.0180, -0.0753,\n",
      "        -0.0187,  0.0677, -0.0622,  0.0232,  0.0467,  0.0355,  0.0778, -0.0188,\n",
      "         0.0638, -0.0271, -0.0804, -0.0059, -0.0371, -0.0710,  0.0061, -0.0845,\n",
      "        -0.0330,  0.0380, -0.0363,  0.0844,  0.0137, -0.0310, -0.0044, -0.0272,\n",
      "         0.0358,  0.0122, -0.0031,  0.0228, -0.0717, -0.0445,  0.0112,  0.0649,\n",
      "        -0.0331,  0.0436, -0.0025,  0.0079,  0.0472,  0.0023, -0.0207, -0.0642,\n",
      "        -0.0789,  0.0138,  0.0068, -0.0405, -0.0552, -0.0665,  0.0686,  0.0504,\n",
      "         0.0428, -0.0874,  0.0847,  0.0770, -0.0510,  0.0735,  0.0560, -0.0335,\n",
      "        -0.0774, -0.0399, -0.0360,  0.0355,  0.0611,  0.0646,  0.0091, -0.0499,\n",
      "        -0.0520,  0.0242, -0.0234, -0.0490,  0.0644,  0.0159,  0.0689,  0.0267,\n",
      "         0.0637,  0.0136, -0.0011,  0.0701,  0.0048,  0.0415, -0.0398,  0.0663,\n",
      "         0.0095,  0.0818,  0.0302,  0.0120, -0.0051,  0.0131,  0.0505,  0.0117,\n",
      "         0.0525,  0.0310,  0.0189,  0.0490, -0.0239,  0.0835,  0.0406,  0.0555,\n",
      "         0.0792,  0.0298, -0.0220,  0.0075, -0.0681,  0.0665, -0.0592,  0.0013,\n",
      "        -0.0442,  0.0507, -0.0429, -0.0686, -0.0665,  0.0445,  0.0286,  0.0793,\n",
      "        -0.0855, -0.0485, -0.0645, -0.0306,  0.0087,  0.0664, -0.0723,  0.0496,\n",
      "        -0.0193, -0.0343,  0.0581, -0.0808, -0.0702,  0.0858,  0.0172, -0.0587,\n",
      "        -0.0720, -0.0649, -0.0831,  0.0014,  0.0842, -0.0266, -0.0528,  0.0727,\n",
      "        -0.0552,  0.0270,  0.0684,  0.0657, -0.0815,  0.0483,  0.0245,  0.0202,\n",
      "        -0.0207, -0.0516, -0.0513, -0.0335, -0.0152, -0.0621, -0.0769, -0.0883,\n",
      "        -0.0833,  0.0356,  0.0425, -0.0543, -0.0067,  0.0109, -0.0685,  0.0783,\n",
      "        -0.0470, -0.0883, -0.0203,  0.0269,  0.0837, -0.0534,  0.0720,  0.0453,\n",
      "         0.0574, -0.0488,  0.0741, -0.0731,  0.0115,  0.0231,  0.0063,  0.0120,\n",
      "         0.0705,  0.0384,  0.0505, -0.0795,  0.0076,  0.0570,  0.0875, -0.0500,\n",
      "        -0.0316, -0.0714,  0.0223, -0.0521, -0.0054, -0.0834, -0.0686, -0.0385,\n",
      "         0.0078, -0.0262,  0.0738,  0.0777,  0.0721,  0.0681, -0.0157, -0.0612,\n",
      "        -0.0295, -0.0672, -0.0038, -0.0342, -0.0473,  0.0868,  0.0591, -0.0129,\n",
      "         0.0502,  0.0507, -0.0109,  0.0505, -0.0640, -0.0137,  0.0365, -0.0405,\n",
      "        -0.0534,  0.0647, -0.0245,  0.0778,  0.0869,  0.0637,  0.0647,  0.0426,\n",
      "        -0.0576, -0.0340,  0.0771,  0.0724, -0.0373, -0.0347,  0.0585,  0.0772,\n",
      "         0.0592,  0.0864, -0.0280, -0.0238, -0.0807,  0.0044,  0.0032,  0.0817,\n",
      "        -0.0524, -0.0836,  0.0733, -0.0743, -0.0395,  0.0645,  0.0649, -0.0343,\n",
      "        -0.0124,  0.0757, -0.0435,  0.0434, -0.0089, -0.0841,  0.0572, -0.0839,\n",
      "         0.0276, -0.0198, -0.0541,  0.0229, -0.0070, -0.0416, -0.0016,  0.0409,\n",
      "         0.0456,  0.0163, -0.0398,  0.0850, -0.0181, -0.0841, -0.0507,  0.0547,\n",
      "        -0.0872,  0.0426,  0.0416,  0.0353, -0.0366,  0.0706, -0.0602,  0.0036,\n",
      "        -0.0393,  0.0145,  0.0375,  0.0421,  0.0214,  0.0589,  0.0846, -0.0579,\n",
      "        -0.0045, -0.0256, -0.0447, -0.0147,  0.0582, -0.0009, -0.0759, -0.0544,\n",
      "         0.0098, -0.0186, -0.0299,  0.0346, -0.0063,  0.0198, -0.0268,  0.0597,\n",
      "        -0.0092,  0.0838, -0.0445,  0.0113, -0.0805,  0.0710, -0.0658, -0.0879,\n",
      "         0.0027,  0.0487,  0.0048, -0.0280, -0.0825,  0.0640,  0.0557,  0.0508,\n",
      "         0.0483,  0.0579,  0.0690, -0.0519, -0.0592,  0.0220,  0.0669,  0.0224,\n",
      "        -0.0436, -0.0075,  0.0842,  0.0755, -0.0422,  0.0716,  0.0215,  0.0117,\n",
      "         0.0571,  0.0607,  0.0605,  0.0105,  0.0094, -0.0831, -0.0359,  0.0499,\n",
      "        -0.0312, -0.0845,  0.0354, -0.0131,  0.0651, -0.0792,  0.0278,  0.0209,\n",
      "        -0.0246, -0.0055, -0.0133,  0.0734, -0.0171, -0.0841,  0.0201,  0.0495,\n",
      "         0.0421, -0.0344,  0.0021,  0.0699, -0.0611, -0.0184, -0.0833,  0.0198,\n",
      "         0.0299,  0.0527, -0.0357, -0.0292,  0.0821,  0.0658,  0.0657,  0.0224,\n",
      "        -0.0784, -0.0762, -0.0601,  0.0120, -0.0311,  0.0386, -0.0726,  0.0875,\n",
      "         0.0528,  0.0537,  0.0511, -0.0158,  0.0062,  0.0618,  0.0437, -0.0646,\n",
      "        -0.0165,  0.0112, -0.0824,  0.0597, -0.0747, -0.0796, -0.0705, -0.0151,\n",
      "        -0.0416, -0.0798,  0.0046,  0.0095,  0.0813,  0.0837, -0.0595, -0.0007,\n",
      "        -0.0062,  0.0667,  0.0698,  0.0703,  0.0644,  0.0291,  0.0814, -0.0859,\n",
      "         0.0416, -0.0626, -0.0012, -0.0016,  0.0531, -0.0118, -0.0783,  0.0294,\n",
      "        -0.0186,  0.0114,  0.0326,  0.0413,  0.0223,  0.0403,  0.0578, -0.0101,\n",
      "        -0.0278,  0.0117,  0.0786,  0.0717,  0.0684,  0.0351, -0.0450, -0.0327,\n",
      "         0.0036,  0.0731,  0.0881, -0.0818, -0.0211,  0.0841, -0.0049, -0.0837,\n",
      "        -0.0078, -0.0708, -0.0116,  0.0080, -0.0766, -0.0830,  0.0343,  0.0657,\n",
      "         0.0578, -0.0376, -0.0240, -0.0842,  0.0118, -0.0320, -0.0766,  0.0110,\n",
      "         0.0034,  0.0714,  0.0608, -0.0494,  0.0681,  0.0327,  0.0512,  0.0100])), ('encoder.lstm.weight_ih_l0_reverse', tensor([[-0.0081,  0.0531,  0.0866,  ...,  0.0816, -0.0598,  0.0800],\n",
      "        [ 0.0055,  0.0342,  0.0879,  ..., -0.0771,  0.0547, -0.0100],\n",
      "        [-0.0017,  0.0484, -0.0001,  ..., -0.0806,  0.0029,  0.0503],\n",
      "        ...,\n",
      "        [ 0.0882, -0.0192, -0.0541,  ..., -0.0373,  0.0028,  0.0735],\n",
      "        [-0.0794,  0.0156,  0.0532,  ...,  0.0236, -0.0690,  0.0132],\n",
      "        [ 0.0661, -0.0758,  0.0612,  ...,  0.0864, -0.0015,  0.0355]])), ('encoder.lstm.weight_hh_l0_reverse', tensor([[-0.0603, -0.0643,  0.0627,  ...,  0.0417, -0.0079,  0.0234],\n",
      "        [-0.0813,  0.0490, -0.0711,  ..., -0.0313, -0.0838,  0.0212],\n",
      "        [-0.0034, -0.0868, -0.0014,  ..., -0.0466,  0.0368,  0.0226],\n",
      "        ...,\n",
      "        [ 0.0254,  0.0227, -0.0531,  ...,  0.0383,  0.0850,  0.0467],\n",
      "        [ 0.0663,  0.0491,  0.0496,  ..., -0.0743,  0.0633, -0.0754],\n",
      "        [-0.0879, -0.0807,  0.0816,  ..., -0.0617, -0.0713, -0.0071]])), ('encoder.lstm.bias_ih_l0_reverse', tensor([ 7.1706e-02,  6.5397e-02, -8.6858e-02,  2.8399e-02,  8.8371e-02,\n",
      "         8.4152e-03, -2.9305e-02, -6.0879e-02, -4.0591e-02, -2.0166e-02,\n",
      "        -2.6600e-02, -3.5963e-02,  1.0323e-02,  7.6030e-02,  5.3039e-02,\n",
      "         4.1044e-02, -8.7319e-02,  6.1698e-02, -3.5201e-02,  5.2303e-02,\n",
      "        -7.1043e-02, -1.3631e-02,  9.5551e-03, -5.1504e-02,  4.4901e-02,\n",
      "         6.8001e-02, -1.6668e-02, -3.7567e-02,  3.0483e-02,  3.7199e-03,\n",
      "        -3.8590e-02,  3.0897e-02,  4.4131e-02, -2.0322e-02, -8.2461e-03,\n",
      "        -6.1656e-02, -5.2257e-03, -2.1823e-02,  1.1880e-02,  5.4140e-02,\n",
      "         5.5604e-04,  6.0017e-02, -2.0929e-02, -8.5012e-02, -6.5727e-02,\n",
      "        -5.1599e-02,  2.9915e-02, -4.0615e-03, -2.3847e-02, -7.3764e-02,\n",
      "         6.2752e-02,  2.6579e-02,  3.3830e-02,  1.4781e-02,  5.1719e-02,\n",
      "        -7.4591e-02,  1.7759e-02, -3.7617e-03, -6.6533e-02,  2.0572e-02,\n",
      "         3.4930e-02, -5.3534e-02, -5.6741e-02,  1.9311e-02,  6.2506e-02,\n",
      "         6.4370e-03,  3.2915e-02, -2.2842e-02,  4.7742e-03, -8.2699e-02,\n",
      "        -1.1466e-02, -3.0721e-04, -1.9762e-02,  3.9178e-02, -4.4765e-02,\n",
      "        -6.4312e-02, -6.4891e-02,  5.8986e-02,  4.0388e-02, -5.8570e-02,\n",
      "        -5.4020e-02,  8.1325e-02, -3.6921e-02,  3.1573e-02,  5.2996e-02,\n",
      "        -6.0256e-02, -7.9319e-02, -2.0305e-02,  4.8799e-02, -1.0471e-02,\n",
      "        -1.2726e-02, -7.1093e-02,  6.6421e-02, -5.3472e-02,  6.7800e-03,\n",
      "        -4.9388e-02, -5.2468e-02, -4.2129e-02,  7.4115e-03,  1.7218e-02,\n",
      "         4.4064e-02,  1.5458e-02, -1.7146e-02,  3.9404e-02,  7.8106e-02,\n",
      "        -8.0224e-02, -6.4693e-02, -2.4559e-02, -1.1676e-02, -1.8376e-02,\n",
      "        -4.8832e-02, -2.4969e-02, -7.7051e-02, -1.6830e-02, -4.0267e-02,\n",
      "         2.8888e-02,  7.1015e-02,  8.1699e-02, -7.2739e-03,  5.6043e-02,\n",
      "        -8.1539e-02, -4.4570e-02, -3.5655e-03, -6.1855e-02,  8.2602e-02,\n",
      "         2.8168e-02,  1.0734e-02,  7.1040e-02, -1.9404e-02,  1.0601e-02,\n",
      "        -1.7027e-02, -5.6205e-02, -6.6661e-02,  4.3901e-02, -4.5389e-03,\n",
      "         5.1826e-02,  2.0107e-02, -4.7241e-02, -7.1723e-02, -7.9358e-02,\n",
      "        -3.4445e-02,  7.9049e-02, -2.2136e-02,  6.9908e-02, -4.6877e-02,\n",
      "         6.6440e-02,  4.8513e-02, -3.4558e-02,  2.1007e-02, -8.1287e-02,\n",
      "        -7.8571e-02,  6.7219e-03, -9.4801e-05, -7.9669e-02, -6.9814e-03,\n",
      "        -3.9906e-02, -4.3794e-02, -5.0296e-02,  5.4928e-02,  8.1902e-02,\n",
      "        -5.1635e-02,  6.1317e-02,  3.4066e-02, -7.9751e-02, -6.8639e-02,\n",
      "         3.8105e-02,  3.0978e-02,  7.3174e-02, -4.7443e-02,  3.8573e-02,\n",
      "        -5.6943e-02,  4.4889e-02, -5.0340e-02,  2.2263e-02,  5.5625e-02,\n",
      "        -6.4924e-02,  8.4885e-02,  1.8415e-02, -2.4778e-02,  3.0564e-02,\n",
      "         1.5060e-02,  5.8513e-02, -6.6456e-02, -5.8243e-03,  8.6619e-02,\n",
      "         6.0437e-02, -7.5145e-02,  8.0783e-02,  1.7919e-02,  8.0786e-02,\n",
      "         2.8428e-02, -1.8264e-02,  4.9881e-02,  7.8894e-02, -3.2324e-03,\n",
      "        -3.5910e-02, -3.4125e-02,  9.6243e-03,  1.3359e-02,  8.2892e-02,\n",
      "        -3.1722e-02,  2.8221e-02,  6.3227e-02, -7.2160e-02, -6.8816e-02,\n",
      "         5.5554e-02, -4.0647e-02,  1.1639e-02, -6.5714e-02, -6.0254e-02,\n",
      "        -7.6979e-02,  3.4151e-02,  5.8021e-03, -4.5708e-02,  5.2868e-02,\n",
      "        -7.5925e-02, -3.3146e-02,  7.8486e-02, -6.5215e-02,  3.9959e-02,\n",
      "         7.0065e-02, -3.7547e-02, -8.5565e-02, -6.7446e-02, -5.2412e-02,\n",
      "         3.6018e-04, -2.6589e-03,  8.4244e-02,  4.9717e-02,  2.9793e-04,\n",
      "        -5.7824e-05,  4.6998e-02, -2.3722e-02,  8.3335e-02,  1.9793e-02,\n",
      "        -5.3412e-02, -2.6118e-02, -7.3649e-02,  2.8574e-02, -5.8283e-02,\n",
      "        -6.4128e-02, -4.6081e-02,  6.1088e-02, -4.4410e-02,  5.2380e-02,\n",
      "        -1.8334e-02,  4.0812e-02, -7.5344e-02,  4.3401e-02, -1.5878e-02,\n",
      "         6.3070e-02, -2.3834e-03, -4.5308e-03, -3.9872e-02,  2.6720e-02,\n",
      "        -8.1903e-02,  6.5737e-03,  9.1195e-03, -4.0103e-02, -7.8652e-02,\n",
      "         1.2813e-02, -1.8393e-02, -4.2275e-02, -5.7694e-02, -7.7905e-02,\n",
      "         5.0315e-02, -7.8153e-02,  4.6233e-02, -3.4089e-02,  7.5986e-02,\n",
      "        -7.3902e-02, -3.6468e-02, -4.0856e-02, -5.1805e-02,  8.4499e-02,\n",
      "         7.4479e-02, -2.3926e-02, -8.7576e-02, -7.9649e-02, -1.7545e-02,\n",
      "        -6.5816e-02,  6.2844e-02, -3.0587e-02, -7.3873e-02, -5.4047e-02,\n",
      "         4.7255e-02, -7.1688e-02, -8.7930e-02, -2.5445e-02, -2.9249e-03,\n",
      "         1.1972e-02,  1.9549e-04, -1.4077e-02, -3.5337e-02,  2.1179e-02,\n",
      "        -6.3693e-02,  7.3030e-02, -8.3456e-02,  4.9347e-02, -1.3111e-02,\n",
      "        -7.5699e-03,  8.3481e-02, -3.9748e-02,  3.9879e-02,  1.7769e-02,\n",
      "         1.9963e-02,  2.5228e-02,  1.0246e-02,  5.5456e-02,  1.5249e-02,\n",
      "         8.3565e-02, -4.6418e-02,  2.9098e-03,  9.8313e-03, -6.1628e-02,\n",
      "        -4.6013e-02, -8.3266e-02, -6.9645e-02,  4.2629e-03,  7.7472e-02,\n",
      "         3.7246e-02, -4.2245e-02, -6.0015e-02,  3.7226e-02, -7.2254e-02,\n",
      "        -4.6178e-02,  9.0114e-03,  1.7727e-02,  7.7740e-02, -2.9903e-02,\n",
      "        -2.2702e-02, -6.6715e-02,  3.1377e-02,  3.6877e-02,  7.3612e-03,\n",
      "         1.5292e-02, -1.2216e-02, -1.3194e-02, -8.6467e-02,  4.5908e-04,\n",
      "        -3.9558e-02,  7.9893e-02,  8.0176e-02,  5.3451e-02,  7.3343e-02,\n",
      "         8.0094e-02, -2.8951e-03,  3.4940e-02,  6.7983e-02, -3.3138e-02,\n",
      "         8.6060e-02, -5.5145e-03,  3.6805e-02,  5.2280e-02, -3.1528e-02,\n",
      "        -4.4680e-02, -7.5992e-02, -7.4974e-03,  7.5562e-03,  7.9409e-02,\n",
      "         5.5036e-02,  6.9692e-02,  8.4247e-03, -4.4663e-02, -5.6496e-02,\n",
      "         7.3873e-02, -3.6215e-02,  8.7665e-02,  5.5261e-02,  2.3895e-03,\n",
      "         6.1642e-02, -2.8907e-02,  3.9057e-02, -3.3425e-02,  1.1215e-02,\n",
      "         1.9508e-03,  2.9069e-02,  1.0568e-02,  6.3544e-02,  4.1906e-02,\n",
      "         6.1536e-02, -5.6847e-02, -7.7295e-02,  8.1163e-02,  8.3379e-03,\n",
      "         1.1935e-02,  7.6186e-02,  4.6681e-02, -8.3662e-02,  4.7836e-03,\n",
      "        -1.7695e-02, -1.2615e-02,  1.8203e-03, -2.3712e-02, -7.4620e-03,\n",
      "        -8.6891e-02,  5.6115e-02, -4.5525e-02, -1.1817e-02, -7.2733e-02,\n",
      "        -8.7487e-02, -6.1442e-02,  6.1447e-02,  8.6268e-02, -8.6193e-02,\n",
      "         7.2789e-02, -3.8741e-02,  5.5396e-02,  1.8015e-02, -1.9197e-02,\n",
      "        -1.2274e-03,  4.8020e-02, -1.7800e-02,  4.6933e-02,  4.9988e-02,\n",
      "        -6.8543e-02,  5.9122e-02, -3.5846e-03, -6.0348e-03, -1.2202e-02,\n",
      "        -3.8192e-02, -1.4850e-02, -8.3390e-02, -3.0577e-02,  7.7646e-02,\n",
      "         3.0219e-02, -7.2035e-02, -7.1358e-02,  1.3236e-02, -3.0796e-02,\n",
      "        -2.9851e-02,  7.9092e-03,  2.3430e-02, -2.7512e-02,  4.2508e-02,\n",
      "         7.2425e-02, -5.5733e-02,  4.4313e-02,  4.2211e-02, -5.1371e-02,\n",
      "        -7.8706e-02,  7.8431e-02, -2.1348e-02,  5.7650e-02,  7.4774e-02,\n",
      "        -2.0439e-02, -3.8673e-02, -5.4941e-02, -2.9358e-02,  5.9873e-02,\n",
      "        -8.7792e-02, -1.5183e-03,  6.7052e-02, -7.4493e-03, -5.2351e-02,\n",
      "        -4.4601e-02, -4.5148e-02, -5.1696e-02, -6.6427e-02, -3.1976e-02,\n",
      "         4.4101e-02,  8.7622e-02,  2.8315e-02,  6.4206e-02,  8.8153e-02,\n",
      "         7.1756e-02, -6.4862e-02,  7.7126e-02,  2.1122e-02,  3.9820e-02,\n",
      "        -6.4399e-02,  3.6469e-02, -2.8909e-03, -4.4319e-02, -8.9675e-03,\n",
      "         2.7999e-02, -2.1275e-02, -7.0364e-02,  8.5169e-02,  4.4980e-02,\n",
      "         6.8438e-02, -5.7972e-02,  7.5625e-02, -2.8232e-02,  4.3443e-02,\n",
      "        -8.5344e-03,  9.1014e-03,  1.1388e-02,  4.0385e-02,  8.7874e-02,\n",
      "        -6.6429e-02,  6.0746e-02, -8.7905e-02, -2.0174e-02, -3.7346e-02,\n",
      "         7.8018e-02,  8.7273e-02,  5.6983e-02, -1.3164e-02, -4.7598e-02,\n",
      "        -1.5227e-02, -8.4919e-02,  9.5838e-03, -6.9014e-02, -4.2233e-02,\n",
      "        -2.8908e-02, -1.8215e-02,  8.4953e-02,  4.0883e-02, -5.5591e-02,\n",
      "        -3.8132e-02,  3.9987e-02])), ('encoder.lstm.bias_hh_l0_reverse', tensor([ 0.0531, -0.0067, -0.0837, -0.0647, -0.0387, -0.0522, -0.0423,  0.0330,\n",
      "         0.0064,  0.0282, -0.0025, -0.0330, -0.0659,  0.0028, -0.0452,  0.0237,\n",
      "         0.0189, -0.0249,  0.0470,  0.0066,  0.0185,  0.0831, -0.0081,  0.0438,\n",
      "         0.0010, -0.0563,  0.0059,  0.0786, -0.0506, -0.0253, -0.0860,  0.0686,\n",
      "        -0.0819, -0.0724,  0.0097, -0.0564,  0.0193, -0.0614, -0.0398, -0.0725,\n",
      "         0.0382,  0.0270, -0.0347, -0.0830,  0.0627, -0.0484,  0.0162,  0.0797,\n",
      "         0.0394,  0.0423, -0.0809, -0.0346,  0.0522, -0.0613,  0.0361, -0.0156,\n",
      "        -0.0334,  0.0552, -0.0853,  0.0133,  0.0768,  0.0601, -0.0189, -0.0295,\n",
      "        -0.0849, -0.0679, -0.0729,  0.0004, -0.0825,  0.0305,  0.0478,  0.0512,\n",
      "         0.0513,  0.0176, -0.0259,  0.0636, -0.0081, -0.0485, -0.0557, -0.0496,\n",
      "        -0.0502,  0.0883, -0.0406,  0.0148,  0.0270, -0.0394, -0.0298, -0.0037,\n",
      "         0.0832, -0.0692, -0.0436, -0.0700, -0.0293, -0.0597,  0.0568, -0.0586,\n",
      "         0.0153, -0.0064, -0.0804,  0.0715,  0.0348, -0.0526,  0.0811, -0.0855,\n",
      "         0.0594,  0.0285,  0.0776, -0.0276,  0.0544,  0.0809,  0.0553,  0.0054,\n",
      "         0.0640, -0.0538,  0.0309, -0.0302,  0.0649,  0.0663, -0.0578,  0.0090,\n",
      "         0.0513,  0.0235, -0.0768, -0.0327, -0.0372, -0.0134,  0.0708, -0.0494,\n",
      "        -0.0691, -0.0586, -0.0042, -0.0551, -0.0844,  0.0261, -0.0216, -0.0133,\n",
      "        -0.0680,  0.0179,  0.0015, -0.0698,  0.0704,  0.0506,  0.0846, -0.0098,\n",
      "        -0.0774,  0.0104,  0.0234, -0.0799,  0.0242, -0.0656, -0.0761, -0.0193,\n",
      "        -0.0745, -0.0225,  0.0091, -0.0728, -0.0481,  0.0098,  0.0408,  0.0066,\n",
      "        -0.0867,  0.0691, -0.0326, -0.0577,  0.0873, -0.0532, -0.0147,  0.0511,\n",
      "         0.0820,  0.0750, -0.0123,  0.0746, -0.0623,  0.0599, -0.0032, -0.0617,\n",
      "         0.0272,  0.0476,  0.0327,  0.0227,  0.0878, -0.0288,  0.0109, -0.0162,\n",
      "         0.0837, -0.0179, -0.0088,  0.0795, -0.0471,  0.0161, -0.0426,  0.0360,\n",
      "        -0.0305,  0.0752, -0.0727, -0.0785,  0.0883, -0.0663,  0.0489, -0.0477,\n",
      "        -0.0776, -0.0034,  0.0045, -0.0759, -0.0003,  0.0616, -0.0721,  0.0006,\n",
      "         0.0337, -0.0365, -0.0170,  0.0564, -0.0256,  0.0212,  0.0818,  0.0353,\n",
      "         0.0594,  0.0809, -0.0607, -0.0310,  0.0521,  0.0715,  0.0854,  0.0678,\n",
      "         0.0120,  0.0419,  0.0052,  0.0220, -0.0799, -0.0822, -0.0692, -0.0191,\n",
      "         0.0572, -0.0162, -0.0544, -0.0112,  0.0379,  0.0158, -0.0249, -0.0047,\n",
      "        -0.0304, -0.0259, -0.0247,  0.0696,  0.0010,  0.0816,  0.0034, -0.0149,\n",
      "         0.0214, -0.0844,  0.0064,  0.0110,  0.0008, -0.0683,  0.0659, -0.0544,\n",
      "         0.0786,  0.0153,  0.0335, -0.0492,  0.0548,  0.0019, -0.0794, -0.0571,\n",
      "         0.0186, -0.0440, -0.0458, -0.0557,  0.0647, -0.0317, -0.0566,  0.0708,\n",
      "         0.0208, -0.0771,  0.0572, -0.0238,  0.0063,  0.0271,  0.0310, -0.0364,\n",
      "        -0.0354, -0.0702,  0.0664, -0.0489, -0.0200, -0.0515,  0.0423,  0.0064,\n",
      "        -0.0330, -0.0344, -0.0730,  0.0176,  0.0352, -0.0543, -0.0086, -0.0664,\n",
      "        -0.0029, -0.0181, -0.0630, -0.0302,  0.0720,  0.0292,  0.0112, -0.0637,\n",
      "         0.0292,  0.0774, -0.0697, -0.0831, -0.0809, -0.0069, -0.0207,  0.0026,\n",
      "        -0.0849,  0.0711,  0.0011, -0.0702,  0.0423,  0.0688,  0.0682,  0.0698,\n",
      "        -0.0016,  0.0056, -0.0232, -0.0182,  0.0356,  0.0122, -0.0546,  0.0502,\n",
      "        -0.0424,  0.0228, -0.0682,  0.0803,  0.0852,  0.0404,  0.0849, -0.0826,\n",
      "         0.0147, -0.0782,  0.0620, -0.0440,  0.0288,  0.0773, -0.0862, -0.0223,\n",
      "         0.0094,  0.0578, -0.0489,  0.0442, -0.0762,  0.0553, -0.0493, -0.0122,\n",
      "         0.0118, -0.0310, -0.0683, -0.0196, -0.0644, -0.0583, -0.0723, -0.0801,\n",
      "         0.0840, -0.0555, -0.0014, -0.0828,  0.0588,  0.0583,  0.0697,  0.0725,\n",
      "        -0.0195, -0.0421,  0.0350, -0.0136, -0.0465,  0.0333,  0.0310,  0.0872,\n",
      "         0.0271, -0.0579,  0.0134,  0.0643,  0.0169,  0.0336,  0.0316, -0.0431,\n",
      "        -0.0201, -0.0563,  0.0506, -0.0525,  0.0476,  0.0578, -0.0518,  0.0076,\n",
      "         0.0114, -0.0546, -0.0257, -0.0247, -0.0319,  0.0411,  0.0178, -0.0161,\n",
      "         0.0264,  0.0126, -0.0161,  0.0574,  0.0463, -0.0412, -0.0793, -0.0088,\n",
      "         0.0102,  0.0091, -0.0036, -0.0246, -0.0792,  0.0271, -0.0545, -0.0325,\n",
      "         0.0772,  0.0563,  0.0599,  0.0539, -0.0041, -0.0760, -0.0219, -0.0441,\n",
      "        -0.0079, -0.0566, -0.0806, -0.0274,  0.0705, -0.0117, -0.0341, -0.0190,\n",
      "        -0.0790,  0.0758, -0.0551, -0.0109,  0.0047,  0.0193,  0.0607,  0.0049,\n",
      "         0.0227, -0.0373,  0.0013,  0.0576, -0.0834,  0.0500,  0.0583, -0.0515,\n",
      "        -0.0233, -0.0362, -0.0345,  0.0135, -0.0170, -0.0237, -0.0161,  0.0770,\n",
      "        -0.0322, -0.0739,  0.0174, -0.0740,  0.0338,  0.0780, -0.0433, -0.0428,\n",
      "        -0.0576, -0.0035,  0.0014,  0.0571,  0.0814, -0.0308,  0.0084, -0.0471,\n",
      "        -0.0234, -0.0136, -0.0172, -0.0142,  0.0506, -0.0266,  0.0187, -0.0737,\n",
      "         0.0265, -0.0035,  0.0196,  0.0263, -0.0426,  0.0493,  0.0433, -0.0639,\n",
      "        -0.0861,  0.0222,  0.0756,  0.0285,  0.0406, -0.0750,  0.0776, -0.0814,\n",
      "        -0.0006,  0.0538, -0.0590, -0.0033, -0.0843,  0.0128, -0.0384, -0.0185,\n",
      "        -0.0572,  0.0077, -0.0057, -0.0676, -0.0875, -0.0414,  0.0026, -0.0689])), ('decoder.embedding.weight', tensor([[-2.1541,  0.4987, -0.4989,  ...,  0.7698, -0.4203, -0.6669],\n",
      "        [ 1.3853,  1.2584, -0.6523,  ...,  1.7814, -0.1848,  0.7033],\n",
      "        [-0.3437,  0.1321,  0.0981,  ..., -1.5938,  0.5758, -1.5283],\n",
      "        ...,\n",
      "        [-2.4812, -0.8758,  1.7863,  ...,  1.7157, -1.3977,  0.9118],\n",
      "        [ 0.2086, -1.0082, -0.1844,  ..., -0.7363, -0.3951, -1.2516],\n",
      "        [ 1.5537,  1.5428, -1.4820,  ...,  2.6536,  0.6411, -1.1650]])), ('decoder.attention.attn.0.weight', tensor([[-0.0373, -0.0446, -0.0306, -0.0251, -0.0219,  0.0138, -0.0351,  0.0347,\n",
      "          0.0220, -0.0136,  0.0186,  0.0413, -0.0277,  0.0505,  0.0498, -0.0120,\n",
      "          0.0435, -0.0386, -0.0452, -0.0189, -0.0349,  0.0430, -0.0092,  0.0361,\n",
      "         -0.0052, -0.0266, -0.0206,  0.0085, -0.0355, -0.0357,  0.0036, -0.0079,\n",
      "          0.0218, -0.0455,  0.0457, -0.0320, -0.0257,  0.0060, -0.0477, -0.0281,\n",
      "         -0.0302,  0.0330, -0.0395, -0.0040,  0.0092, -0.0304,  0.0438, -0.0008,\n",
      "         -0.0448,  0.0143,  0.0299, -0.0274,  0.0404, -0.0234, -0.0278,  0.0041,\n",
      "          0.0090,  0.0341, -0.0318, -0.0072, -0.0267,  0.0262, -0.0399, -0.0324,\n",
      "          0.0158, -0.0337,  0.0510,  0.0416,  0.0425, -0.0309,  0.0069,  0.0360,\n",
      "          0.0282,  0.0243,  0.0368,  0.0427,  0.0419, -0.0225, -0.0182, -0.0495,\n",
      "          0.0368, -0.0115,  0.0168, -0.0231,  0.0447,  0.0025,  0.0228,  0.0110,\n",
      "         -0.0137, -0.0413, -0.0151,  0.0278, -0.0308,  0.0111, -0.0178,  0.0369,\n",
      "         -0.0404,  0.0434, -0.0289,  0.0332, -0.0321,  0.0389,  0.0194,  0.0279,\n",
      "          0.0004, -0.0167,  0.0492, -0.0248,  0.0414,  0.0107,  0.0015, -0.0503,\n",
      "         -0.0259, -0.0056, -0.0105, -0.0139,  0.0113, -0.0349,  0.0260,  0.0289,\n",
      "          0.0420,  0.0332,  0.0315, -0.0475,  0.0002,  0.0209,  0.0136,  0.0083,\n",
      "         -0.0151, -0.0304,  0.0110, -0.0129,  0.0176, -0.0312,  0.0366, -0.0339,\n",
      "         -0.0154,  0.0020, -0.0331, -0.0131,  0.0072,  0.0261,  0.0509, -0.0181,\n",
      "          0.0315, -0.0145, -0.0290, -0.0431, -0.0366, -0.0474, -0.0133, -0.0261,\n",
      "         -0.0417,  0.0084,  0.0486,  0.0095,  0.0124,  0.0308, -0.0051, -0.0032,\n",
      "          0.0236,  0.0344, -0.0420, -0.0278, -0.0321, -0.0186,  0.0367, -0.0007,\n",
      "          0.0275,  0.0316,  0.0500, -0.0407, -0.0351,  0.0187, -0.0082,  0.0419,\n",
      "         -0.0356,  0.0308, -0.0165, -0.0019, -0.0176,  0.0449,  0.0266, -0.0311,\n",
      "          0.0091, -0.0450, -0.0021, -0.0034, -0.0128, -0.0083, -0.0473,  0.0436,\n",
      "         -0.0231,  0.0057,  0.0031,  0.0243,  0.0383,  0.0210, -0.0425,  0.0113,\n",
      "         -0.0177, -0.0487, -0.0249,  0.0159,  0.0220,  0.0014,  0.0317, -0.0022,\n",
      "         -0.0450,  0.0389,  0.0333,  0.0286,  0.0303,  0.0452,  0.0262,  0.0484,\n",
      "         -0.0243,  0.0044, -0.0339, -0.0036, -0.0194, -0.0166,  0.0238,  0.0297,\n",
      "         -0.0133,  0.0229,  0.0157, -0.0385,  0.0239, -0.0336,  0.0414,  0.0496,\n",
      "         -0.0441,  0.0053, -0.0052,  0.0285, -0.0299,  0.0382,  0.0451, -0.0034,\n",
      "         -0.0076, -0.0461, -0.0171, -0.0049, -0.0056, -0.0259, -0.0032, -0.0356,\n",
      "         -0.0280, -0.0496,  0.0214,  0.0174, -0.0223,  0.0193,  0.0029,  0.0370,\n",
      "          0.0469, -0.0270,  0.0446, -0.0180, -0.0453,  0.0061, -0.0384,  0.0104,\n",
      "         -0.0043, -0.0130,  0.0341, -0.0445,  0.0056, -0.0346,  0.0471,  0.0383,\n",
      "          0.0004, -0.0144, -0.0140,  0.0017,  0.0181, -0.0222,  0.0113, -0.0318,\n",
      "         -0.0169, -0.0243, -0.0014,  0.0492,  0.0267,  0.0030, -0.0011, -0.0445,\n",
      "         -0.0300,  0.0209, -0.0334,  0.0143,  0.0232,  0.0358, -0.0042, -0.0497,\n",
      "          0.0433,  0.0504,  0.0259,  0.0406, -0.0423,  0.0014, -0.0358,  0.0479,\n",
      "          0.0104,  0.0033,  0.0384,  0.0125, -0.0029, -0.0075, -0.0017,  0.0499,\n",
      "          0.0180,  0.0388,  0.0475, -0.0090, -0.0061, -0.0479, -0.0218, -0.0274,\n",
      "          0.0141, -0.0504, -0.0384,  0.0206, -0.0061,  0.0140,  0.0077, -0.0342,\n",
      "         -0.0077,  0.0239,  0.0312, -0.0447, -0.0425,  0.0262, -0.0176, -0.0095,\n",
      "         -0.0114, -0.0027,  0.0293,  0.0279, -0.0114, -0.0434, -0.0108, -0.0342,\n",
      "         -0.0422, -0.0319,  0.0269, -0.0216, -0.0349,  0.0162, -0.0463, -0.0004,\n",
      "          0.0131,  0.0335, -0.0453, -0.0406,  0.0369,  0.0245,  0.0145, -0.0405,\n",
      "          0.0457,  0.0147, -0.0120,  0.0321,  0.0129, -0.0134, -0.0008, -0.0466,\n",
      "         -0.0167, -0.0216,  0.0062,  0.0284, -0.0158,  0.0420, -0.0459,  0.0096,\n",
      "         -0.0008,  0.0416, -0.0010,  0.0345,  0.0474,  0.0445,  0.0470,  0.0136]])), ('decoder.attention.attn.0.bias', tensor([0.0284])), ('decoder.lstm.weight_ih_l0', tensor([[-8.7597e-02, -2.3791e-02, -4.1109e-02,  ..., -6.1256e-02,\n",
      "          3.2825e-02, -7.4134e-02],\n",
      "        [ 8.2033e-02,  2.5778e-02,  9.0427e-03,  ..., -5.4896e-02,\n",
      "         -5.1487e-02, -1.4024e-02],\n",
      "        [ 8.7713e-02, -5.9880e-02,  4.9225e-02,  ...,  5.6953e-03,\n",
      "          8.5698e-02, -4.9126e-02],\n",
      "        ...,\n",
      "        [ 7.1965e-05,  4.6414e-02,  8.1878e-02,  ...,  5.2639e-02,\n",
      "          6.9991e-02, -4.1154e-02],\n",
      "        [ 2.9708e-03, -5.5293e-02, -3.5986e-02,  ..., -6.8027e-03,\n",
      "          2.2072e-02,  3.6143e-03],\n",
      "        [ 1.0942e-02, -2.3662e-02, -8.8516e-03,  ...,  3.0961e-02,\n",
      "          8.2464e-02, -4.8968e-02]])), ('decoder.lstm.weight_hh_l0', tensor([[ 0.0400,  0.0554,  0.0841,  ..., -0.0182,  0.0605, -0.0685],\n",
      "        [ 0.0378, -0.0071,  0.0659,  ..., -0.0471, -0.0242, -0.0273],\n",
      "        [ 0.0373, -0.0657, -0.0559,  ...,  0.0663,  0.0423,  0.0122],\n",
      "        ...,\n",
      "        [-0.0793, -0.0492,  0.0527,  ..., -0.0201, -0.0598,  0.0490],\n",
      "        [ 0.0567, -0.0647,  0.0603,  ...,  0.0291, -0.0052,  0.0604],\n",
      "        [ 0.0246, -0.0184, -0.0625,  ..., -0.0656,  0.0064, -0.0807]])), ('decoder.lstm.bias_ih_l0', tensor([ 2.8653e-02,  5.0692e-02, -2.8594e-02,  2.3321e-02,  7.1481e-02,\n",
      "        -6.8686e-02, -5.2513e-02, -1.2967e-02, -7.2949e-03, -1.8810e-02,\n",
      "        -1.6992e-02,  3.8960e-02,  6.9957e-02,  7.6353e-02,  3.8578e-02,\n",
      "         4.0595e-02,  2.4696e-02,  8.5089e-02, -7.3162e-02,  3.5625e-02,\n",
      "         4.7320e-02, -2.7880e-02, -2.7023e-02,  6.8917e-02, -8.1473e-02,\n",
      "        -7.0709e-02, -2.5157e-02,  3.6272e-02, -4.9285e-03,  1.4791e-02,\n",
      "         4.9108e-02,  3.4163e-02,  3.6656e-02, -6.9216e-02,  4.7844e-02,\n",
      "         6.6385e-02,  5.2889e-02,  1.8967e-03,  5.6352e-02,  8.7545e-02,\n",
      "         2.8395e-02,  3.2744e-02, -3.8796e-02, -1.2100e-02, -7.0898e-02,\n",
      "        -4.2918e-02,  2.5722e-02, -1.0956e-02,  1.7927e-03, -5.0310e-02,\n",
      "        -5.5712e-02, -4.7252e-02,  3.8195e-02,  8.0779e-02,  2.3537e-02,\n",
      "        -4.0647e-02, -8.3470e-02,  1.1694e-02, -1.3282e-03,  8.4852e-02,\n",
      "        -3.3855e-02, -2.2664e-02,  1.6351e-02, -6.1690e-02,  7.3808e-02,\n",
      "         7.9832e-02,  4.7682e-02,  1.0002e-02, -8.4750e-02, -2.3336e-03,\n",
      "        -8.3451e-02,  6.6851e-03, -5.5295e-02, -8.4189e-02, -2.0256e-02,\n",
      "        -6.4429e-02, -6.7934e-02,  8.4882e-02, -1.5141e-02,  3.8452e-02,\n",
      "        -5.7501e-02, -4.8982e-02, -5.6556e-02, -5.4653e-02, -5.0377e-02,\n",
      "         8.0642e-03,  2.2209e-02,  1.3215e-02,  2.4498e-02,  5.9495e-02,\n",
      "         2.0292e-02,  6.9612e-02,  5.4396e-02,  3.5582e-02,  5.9018e-02,\n",
      "        -4.9870e-02,  5.3036e-02,  9.2215e-04, -8.8967e-03, -7.8266e-02,\n",
      "         3.3981e-02,  4.7607e-02,  6.5970e-02,  3.3411e-02,  4.5721e-02,\n",
      "         3.7949e-02,  3.6700e-02, -3.8387e-02,  4.1253e-02, -8.2500e-02,\n",
      "         7.8241e-02,  5.2997e-02, -4.7410e-02,  7.4685e-02, -7.6048e-02,\n",
      "         8.1450e-03, -1.5472e-02,  6.0008e-03,  3.0548e-02,  6.3460e-03,\n",
      "        -5.4463e-02,  3.3635e-02,  3.5225e-02,  4.7705e-02,  8.0201e-02,\n",
      "         2.2028e-03,  5.8374e-02, -2.9199e-02, -8.0671e-04, -4.2627e-02,\n",
      "        -3.1132e-02,  8.4824e-03, -5.9676e-02, -8.0225e-02,  3.0703e-02,\n",
      "        -1.5302e-02, -7.6075e-02,  5.0083e-02, -5.6465e-02,  8.4867e-02,\n",
      "        -8.4985e-02, -6.1020e-05, -5.2733e-02,  8.7961e-02, -5.5197e-02,\n",
      "        -2.3513e-02,  8.4451e-02,  1.8485e-02,  3.9578e-02, -5.1013e-02,\n",
      "         1.8981e-02,  1.2605e-02,  5.4158e-02,  7.4736e-02,  5.2402e-02,\n",
      "        -3.3342e-02, -1.5688e-02,  2.5915e-02,  4.3693e-02, -8.4789e-02,\n",
      "         3.5979e-02,  7.1168e-02,  7.5714e-02,  4.3548e-02, -6.7847e-03,\n",
      "        -2.4883e-02,  4.8125e-02, -2.4403e-02, -8.8285e-02, -2.0368e-02,\n",
      "         4.2338e-02, -7.0794e-02, -1.5678e-02, -8.5812e-02, -1.7824e-02,\n",
      "         3.6346e-02,  9.9105e-03,  4.2801e-04, -3.5244e-02, -3.6584e-02,\n",
      "         2.4542e-02,  1.3471e-02, -8.2622e-02,  2.4960e-02,  3.8922e-03,\n",
      "        -3.5758e-02,  7.8914e-02, -5.3830e-02,  6.4611e-02,  2.5379e-02,\n",
      "         5.4956e-02, -8.3344e-02, -5.4826e-02,  7.3654e-02, -8.2703e-02,\n",
      "        -4.6599e-02,  7.0382e-02, -2.9930e-02,  5.5741e-02, -4.0137e-02,\n",
      "         7.1869e-02, -6.1939e-02, -2.6873e-02,  4.0982e-02,  1.3156e-02,\n",
      "        -7.3238e-02, -8.6104e-03, -3.8622e-02, -7.1731e-03, -5.6313e-02,\n",
      "         3.1070e-02, -5.2283e-02,  7.5008e-02,  6.4959e-02,  1.9158e-02,\n",
      "        -2.3569e-02, -1.6614e-03,  2.9003e-02, -6.5776e-02, -4.2498e-02,\n",
      "         1.4414e-02, -7.8159e-02,  5.8045e-02,  5.4941e-02,  3.5204e-03,\n",
      "        -8.2156e-02, -2.9528e-03,  3.7896e-03,  6.2063e-02,  4.4349e-02,\n",
      "        -6.5364e-02,  1.2815e-02, -7.2775e-02,  6.3084e-02,  7.4751e-02,\n",
      "        -8.0167e-03,  2.3137e-02,  2.0320e-02,  4.4473e-02,  2.2425e-02,\n",
      "         2.8823e-02,  1.4084e-02,  3.8171e-03,  2.9792e-02,  5.9302e-02,\n",
      "        -6.3714e-02,  4.2520e-02, -2.5949e-03,  8.8117e-02,  3.9678e-02,\n",
      "        -4.5729e-02, -4.5090e-02,  4.5544e-02,  5.2890e-02,  1.9708e-02,\n",
      "        -8.1389e-02,  2.2483e-02, -2.3649e-02,  5.9798e-02, -3.6226e-02,\n",
      "         7.4376e-02,  7.8650e-02,  2.2091e-02, -6.2218e-02, -4.6949e-02,\n",
      "        -2.5492e-02, -4.5011e-02, -3.1104e-02,  7.9342e-02,  1.7852e-02,\n",
      "        -7.1018e-02, -1.9643e-02,  5.9406e-02,  4.8729e-02, -1.0115e-02,\n",
      "        -5.7029e-04,  4.8320e-02, -5.1041e-02,  4.0896e-02,  2.7263e-02,\n",
      "         6.1301e-02,  6.8374e-02, -6.6374e-02, -8.2063e-02,  2.2305e-03,\n",
      "        -3.3933e-02,  1.9886e-02, -4.3316e-02, -3.8789e-02,  6.0952e-02,\n",
      "        -5.9318e-02, -8.2269e-02, -5.6762e-02,  5.1368e-02, -6.5316e-02,\n",
      "         1.6321e-02,  5.1090e-02,  8.1491e-02,  4.6907e-02, -6.1946e-02,\n",
      "         6.1950e-02, -1.2030e-03,  5.3103e-02, -6.0921e-03, -3.5456e-02,\n",
      "         3.2775e-02,  6.2365e-02,  6.6235e-02, -1.2816e-02, -8.1105e-02,\n",
      "        -3.5281e-02, -6.2851e-02, -3.1586e-02,  5.5406e-02,  7.8510e-02,\n",
      "         1.9361e-02,  5.1475e-02,  6.8446e-02, -7.6601e-02, -7.1530e-02,\n",
      "        -3.4011e-02,  5.0087e-02,  7.7180e-02, -2.5941e-02, -4.4127e-02,\n",
      "        -2.3191e-02,  6.4812e-02,  6.7138e-02,  1.4401e-02, -6.3862e-02,\n",
      "         4.6087e-02, -8.2241e-03, -2.9909e-02,  1.8490e-03,  6.1964e-02,\n",
      "         6.2349e-02, -2.7787e-02,  8.4204e-03, -3.7513e-02, -6.1154e-02,\n",
      "         1.8671e-02,  2.5814e-02, -7.1904e-02, -4.7007e-02,  2.0353e-02,\n",
      "        -7.6103e-02, -2.1707e-02,  3.9893e-02,  3.9617e-02,  2.4178e-02,\n",
      "        -4.0773e-02,  1.6218e-03,  8.7363e-02, -8.6682e-02,  3.8948e-02,\n",
      "        -5.8463e-02, -8.5896e-02, -7.3193e-02, -3.6127e-02, -7.3240e-02,\n",
      "         6.6923e-02,  4.4479e-02, -7.0158e-02,  4.5664e-02, -3.2783e-02,\n",
      "        -3.1730e-02,  2.4934e-02, -3.1970e-02,  5.4855e-02,  8.1901e-02,\n",
      "         2.3544e-02,  7.9627e-02, -8.5198e-03, -6.3536e-02,  4.8782e-02,\n",
      "         7.7233e-02, -5.3712e-02, -2.1771e-02,  1.6395e-02,  7.5509e-02,\n",
      "         4.5893e-02,  4.3593e-02,  6.7062e-02, -7.9460e-02, -7.7847e-02,\n",
      "        -8.3743e-02,  8.4441e-02,  1.9666e-02, -6.2495e-02,  2.9118e-02,\n",
      "        -7.3655e-02, -2.4310e-02,  3.9429e-02, -1.4066e-02,  4.2975e-03,\n",
      "        -2.5545e-02,  8.2344e-02,  1.2173e-03, -4.4252e-02, -4.7199e-02,\n",
      "         1.6846e-02,  3.0511e-03, -8.7584e-02,  6.0475e-02, -8.3110e-02,\n",
      "         5.9729e-02,  5.0727e-02, -3.2789e-02,  5.7002e-02, -6.5523e-02,\n",
      "         4.6508e-02,  5.0902e-02,  1.8827e-02, -6.7159e-02,  4.9512e-02,\n",
      "         8.6885e-02, -6.0717e-02,  1.9449e-02, -5.3268e-02, -7.2510e-02,\n",
      "         6.4330e-02, -5.8251e-02, -6.0425e-02, -1.4380e-02,  4.9864e-02,\n",
      "        -1.6863e-02, -5.4859e-02,  5.4659e-02, -4.8552e-02, -8.4214e-02,\n",
      "         5.0830e-02, -1.3413e-02, -5.1262e-02,  6.9047e-02, -9.5625e-03,\n",
      "         7.1898e-02, -1.9582e-02,  6.3966e-02,  1.7516e-03,  2.0394e-02,\n",
      "        -8.1264e-03, -1.1575e-02, -4.7750e-02,  6.0295e-02, -5.8611e-02,\n",
      "        -7.9295e-02,  6.8244e-02,  3.8683e-04, -1.1733e-02,  2.2890e-03,\n",
      "         7.3364e-02, -5.0861e-02, -5.2515e-03,  2.8221e-02, -5.4904e-02,\n",
      "         2.4588e-02, -4.2836e-02, -5.8865e-02,  3.3351e-02, -7.5290e-02,\n",
      "         1.7831e-02,  6.5885e-02,  6.9681e-02, -1.0661e-02, -3.8046e-02,\n",
      "         5.5025e-02, -1.2102e-02,  8.1203e-02,  6.0782e-02,  1.0746e-02,\n",
      "        -3.5048e-02, -1.9360e-02,  7.6254e-02,  6.0662e-02, -3.0044e-02,\n",
      "        -7.0131e-02, -5.8119e-02,  5.3205e-02,  7.6894e-02,  2.6678e-02,\n",
      "        -4.1294e-02,  4.1667e-02,  1.0634e-02, -8.2851e-02, -2.8767e-03,\n",
      "         3.9999e-02, -7.4639e-03,  2.8341e-02, -1.0059e-02, -3.9412e-02,\n",
      "        -5.7442e-02,  6.2098e-02, -4.9969e-02,  6.7631e-02, -7.0858e-02,\n",
      "         6.6265e-02,  6.9831e-03, -7.9405e-02, -5.9750e-03,  3.7599e-02,\n",
      "         6.6093e-02,  2.7311e-02, -6.7705e-02,  6.1263e-02, -3.7471e-02,\n",
      "         3.3262e-03,  6.8241e-03,  6.7701e-02,  7.1431e-02,  5.0617e-02,\n",
      "        -3.6586e-03,  8.6238e-02])), ('decoder.lstm.bias_hh_l0', tensor([-0.0629,  0.0563, -0.0575, -0.0819,  0.0871, -0.0199, -0.0863, -0.0567,\n",
      "        -0.0501,  0.0631,  0.0677, -0.0249,  0.0454,  0.0565, -0.0095,  0.0295,\n",
      "         0.0280,  0.0556,  0.0567,  0.0337, -0.0732, -0.0125,  0.0061,  0.0238,\n",
      "         0.0349, -0.0848,  0.0377, -0.0089,  0.0491,  0.0784, -0.0804, -0.0210,\n",
      "         0.0054, -0.0592,  0.0550, -0.0781, -0.0485, -0.0779,  0.0858, -0.0431,\n",
      "        -0.0035,  0.0510, -0.0689,  0.0399,  0.0425,  0.0814,  0.0648, -0.0042,\n",
      "        -0.0224,  0.0297, -0.0413,  0.0145,  0.0340,  0.0288,  0.0659, -0.0345,\n",
      "        -0.0063,  0.0295, -0.0203,  0.0543,  0.0477,  0.0670,  0.0341,  0.0531,\n",
      "        -0.0819,  0.0145,  0.0697, -0.0146, -0.0757, -0.0011, -0.0637,  0.0799,\n",
      "        -0.0770,  0.0573,  0.0668, -0.0172, -0.0291, -0.0771, -0.0343, -0.0091,\n",
      "         0.0378,  0.0073, -0.0149,  0.0348, -0.0580,  0.0262,  0.0756,  0.0772,\n",
      "         0.0058,  0.0461, -0.0409, -0.0380, -0.0090,  0.0108,  0.0227,  0.0525,\n",
      "         0.0096,  0.0637, -0.0310, -0.0712, -0.0859,  0.0096,  0.0434,  0.0836,\n",
      "        -0.0397,  0.0113,  0.0363, -0.0152,  0.0278, -0.0149,  0.0626, -0.0058,\n",
      "         0.0517,  0.0656, -0.0819, -0.0181,  0.0270, -0.0797,  0.0804, -0.0294,\n",
      "         0.0080,  0.0655, -0.0881, -0.0633, -0.0045,  0.0849,  0.0661, -0.0622,\n",
      "        -0.0854,  0.0141, -0.0577,  0.0410,  0.0812, -0.0657, -0.0057,  0.0860,\n",
      "        -0.0248,  0.0008,  0.0793,  0.0548,  0.0169,  0.0539, -0.0757, -0.0336,\n",
      "        -0.0880,  0.0848, -0.0257, -0.0367,  0.0030,  0.0784,  0.0199,  0.0142,\n",
      "         0.0085, -0.0203, -0.0259,  0.0422, -0.0880,  0.0748, -0.0491,  0.0848,\n",
      "         0.0617, -0.0043, -0.0348,  0.0165,  0.0828,  0.0558, -0.0346, -0.0507,\n",
      "        -0.0729, -0.0644,  0.0882,  0.0354, -0.0035,  0.0521, -0.0763, -0.0490,\n",
      "        -0.0784, -0.0491,  0.0162, -0.0238, -0.0469,  0.0422,  0.0319, -0.0313,\n",
      "        -0.0487,  0.0295, -0.0526, -0.0101, -0.0378, -0.0753, -0.0303, -0.0578,\n",
      "         0.0593, -0.0849, -0.0678, -0.0115,  0.0744,  0.0468, -0.0200, -0.0556,\n",
      "         0.0779, -0.0341,  0.0403,  0.0630, -0.0508,  0.0225,  0.0620, -0.0320,\n",
      "        -0.0153,  0.0382, -0.0580, -0.0499, -0.0569,  0.0546,  0.0736,  0.0443,\n",
      "         0.0669, -0.0348, -0.0298, -0.0716,  0.0798,  0.0458,  0.0434,  0.0470,\n",
      "         0.0416,  0.0504, -0.0644, -0.0637,  0.0196,  0.0562, -0.0392,  0.0756,\n",
      "        -0.0137, -0.0094, -0.0360, -0.0019,  0.0670,  0.0454,  0.0470,  0.0865,\n",
      "         0.0178, -0.0013,  0.0127,  0.0088, -0.0513,  0.0486,  0.0506, -0.0111,\n",
      "         0.0745, -0.0489, -0.0838,  0.0848, -0.0206, -0.0345,  0.0300, -0.0473,\n",
      "         0.0700, -0.0670,  0.0403,  0.0094, -0.0720, -0.0561, -0.0398,  0.0804,\n",
      "        -0.0492, -0.0579, -0.0675,  0.0270,  0.0619, -0.0663,  0.0116,  0.0094,\n",
      "         0.0682, -0.0360, -0.0207, -0.0379, -0.0879,  0.0349,  0.0125,  0.0237,\n",
      "        -0.0435, -0.0561, -0.0179,  0.0856,  0.0434,  0.0159, -0.0483, -0.0368,\n",
      "         0.0182,  0.0445,  0.0735,  0.0268, -0.0535, -0.0071, -0.0088, -0.0332,\n",
      "         0.0841,  0.0487, -0.0098,  0.0199,  0.0074, -0.0768, -0.0608, -0.0151,\n",
      "         0.0097,  0.0079, -0.0102, -0.0783,  0.0171,  0.0001, -0.0417, -0.0449,\n",
      "        -0.0464, -0.0294,  0.0599, -0.0381, -0.0190, -0.0482, -0.0697, -0.0451,\n",
      "         0.0072,  0.0592, -0.0347, -0.0615, -0.0241,  0.0794, -0.0048, -0.0061,\n",
      "         0.0525, -0.0768,  0.0608, -0.0788,  0.0213,  0.0834,  0.0132,  0.0126,\n",
      "        -0.0129, -0.0138,  0.0460, -0.0020, -0.0750,  0.0309,  0.0263,  0.0534,\n",
      "        -0.0679, -0.0794, -0.0126, -0.0148,  0.0427,  0.0390, -0.0069, -0.0283,\n",
      "        -0.0177, -0.0788, -0.0499,  0.0818, -0.0445,  0.0362,  0.0627,  0.0568,\n",
      "         0.0278,  0.0395, -0.0535,  0.0110, -0.0219,  0.0597, -0.0115,  0.0504,\n",
      "         0.0201, -0.0570, -0.0420, -0.0191, -0.0008, -0.0703, -0.0539,  0.0177,\n",
      "         0.0285,  0.0213, -0.0675, -0.0143,  0.0824, -0.0461, -0.0068, -0.0617,\n",
      "        -0.0741,  0.0404, -0.0611,  0.0483,  0.0122,  0.0636, -0.0223, -0.0650,\n",
      "         0.0023, -0.0049,  0.0153, -0.0366,  0.0547, -0.0400,  0.0089,  0.0883,\n",
      "         0.0253, -0.0282,  0.0763,  0.0388,  0.0256,  0.0024, -0.0024, -0.0059,\n",
      "        -0.0729, -0.0332,  0.0649,  0.0713,  0.0835, -0.0804, -0.0037, -0.0225,\n",
      "         0.0389,  0.0830, -0.0881, -0.0506,  0.0859, -0.0526,  0.0460,  0.0832,\n",
      "        -0.0187, -0.0441, -0.0494, -0.0823, -0.0336, -0.0371,  0.0819, -0.0287,\n",
      "         0.0461,  0.0333, -0.0309, -0.0485,  0.0685,  0.0370,  0.0473,  0.0071,\n",
      "         0.0846, -0.0691, -0.0426, -0.0720, -0.0744, -0.0367, -0.0479, -0.0026,\n",
      "        -0.0492,  0.0352, -0.0290,  0.0207,  0.0148, -0.0569,  0.0196,  0.0604,\n",
      "         0.0195,  0.0016,  0.0282, -0.0507,  0.0550, -0.0323,  0.0788, -0.0243,\n",
      "        -0.0776,  0.0805, -0.0608,  0.0287,  0.0066, -0.0260,  0.0806,  0.0048,\n",
      "         0.0502, -0.0090,  0.0669, -0.0304, -0.0107, -0.0776, -0.0497,  0.0610,\n",
      "         0.0402,  0.0718, -0.0759, -0.0180, -0.0722, -0.0448, -0.0529, -0.0119,\n",
      "        -0.0301, -0.0682,  0.0307, -0.0578,  0.0669, -0.0646, -0.0624,  0.0530,\n",
      "         0.0770, -0.0091, -0.0507,  0.0121, -0.0150,  0.0355, -0.0251,  0.0486,\n",
      "         0.0312, -0.0074,  0.0387,  0.0794, -0.0581,  0.0607, -0.0466, -0.0809])), ('decoder.fc_model.0.weight', tensor([[ 0.0138,  0.0072,  0.0644,  ...,  0.0428, -0.0136, -0.0425],\n",
      "        [ 0.0694, -0.0357,  0.0481,  ...,  0.0478, -0.0702, -0.0767],\n",
      "        [-0.0070, -0.0386, -0.0619,  ...,  0.0405,  0.0309, -0.0373],\n",
      "        ...,\n",
      "        [ 0.0814,  0.0854, -0.0457,  ...,  0.0688,  0.0574, -0.0259],\n",
      "        [-0.0762,  0.0864, -0.0551,  ...,  0.0065, -0.0290,  0.0639],\n",
      "        [ 0.0018, -0.0559,  0.0757,  ..., -0.0530,  0.0827, -0.0194]])), ('decoder.fc_model.0.bias', tensor([ 0.0621, -0.0285, -0.0588, -0.0016,  0.0527, -0.0233,  0.0128,  0.0729,\n",
      "        -0.0821, -0.0882, -0.0299, -0.0028,  0.0330, -0.0863,  0.0370, -0.0759,\n",
      "         0.0394, -0.0540, -0.0306, -0.0729, -0.0363,  0.0145,  0.0583, -0.0258,\n",
      "         0.0043, -0.0038, -0.0492, -0.0715,  0.0724,  0.0383,  0.0702, -0.0107,\n",
      "        -0.0771, -0.0530, -0.0150, -0.0707, -0.0520,  0.0319, -0.0346, -0.0011,\n",
      "         0.0049, -0.0175,  0.0259,  0.0862,  0.0295, -0.0828,  0.0769, -0.0277,\n",
      "         0.0547,  0.0285, -0.0160, -0.0839,  0.0708,  0.0500,  0.0788, -0.0606,\n",
      "        -0.0808,  0.0341, -0.0351,  0.0452, -0.0368,  0.0710, -0.0698,  0.0719,\n",
      "         0.0722,  0.0205,  0.0715, -0.0877, -0.0357,  0.0124,  0.0090,  0.0050,\n",
      "        -0.0520, -0.0401, -0.0878,  0.0275,  0.0327,  0.0579,  0.0263,  0.0664,\n",
      "         0.0429,  0.0519, -0.0620,  0.0693,  0.0456,  0.0642, -0.0788, -0.0140,\n",
      "        -0.0059,  0.0419, -0.0055, -0.0233, -0.0280, -0.0155,  0.0592,  0.0398,\n",
      "         0.0156, -0.0198, -0.0184, -0.0069,  0.0551,  0.0426,  0.0778, -0.0859,\n",
      "         0.0131, -0.0033,  0.0010,  0.0327, -0.0778, -0.0410,  0.0343, -0.0710,\n",
      "         0.0763, -0.0421,  0.0342,  0.0765,  0.0329,  0.0278,  0.0647, -0.0346,\n",
      "         0.0597, -0.0564,  0.0878,  0.0691,  0.0578, -0.0615, -0.0438, -0.0828])), ('decoder.fc_model.1.weight', tensor([[-0.0116,  0.0737, -0.0392,  ..., -0.0261,  0.0086, -0.0526],\n",
      "        [-0.0760, -0.0796,  0.0596,  ..., -0.0373, -0.0364,  0.0367],\n",
      "        [ 0.0834, -0.0703,  0.0340,  ..., -0.0679,  0.0595,  0.0791],\n",
      "        ...,\n",
      "        [-0.0545,  0.0184,  0.0834,  ..., -0.0302,  0.0330,  0.0727],\n",
      "        [-0.0329,  0.0714, -0.0776,  ...,  0.0793, -0.0603, -0.0699],\n",
      "        [ 0.0274,  0.0697,  0.0330,  ..., -0.0690, -0.0038,  0.0333]])), ('decoder.fc_model.1.bias', tensor([-0.0214, -0.0672, -0.0841,  ...,  0.0547, -0.0487,  0.0369]))])\n"
     ]
    }
   ],
   "source": [
    "# Define all the parameters for the model\n",
    "\n",
    "dir = os.path.join(os.curdir, 'cornell-movie-dialogs-corpus/movie_lines.txt')\n",
    "idx_to_text = get_movie_lines(dir)\n",
    "# create a list of dialogs for each movie.\n",
    "dialogs = extract_dialogs()\n",
    "# for each movie, create pairs dialogs (Q/A). This is the actual data used for training.\n",
    "pair_dialogs_idx = create_pair_dialogs(dialogs)\n",
    "# limit pairs for batch building\n",
    "pair_dialogs_idx = select_n_pairs(pair_dialogs_idx, 1000)  # 100000\n",
    "# instantiate the vocabulary\n",
    "vocabulary = Vocabulary(idx_to_text, dialogs)\n",
    "print('Total words counted in the vocabulary: {}'.format(vocabulary.__len__()))\n",
    "\n",
    "# create the dataset class to store the data\n",
    "train_data = CornellCorpus(pair_dialogs_idx, vocabulary, train_data=True)\n",
    "val_data = CornellCorpus(pair_dialogs_idx, vocabulary, train_data=False)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 512\n",
    "hidden_size = 128\n",
    "embedding_size = 300\n",
    "epochs = 10\n",
    "optim_parameters = {'lr': 1e-5, 'weight_decay': 1e-3}\n",
    "\n",
    "# init dataloader\n",
    "load_args = {'batch_size': batch_size, 'shuffle': True}\n",
    "train_dataloader = DataLoader(train_data, **load_args)\n",
    "val_dataloader = DataLoader(val_data, **load_args)\n",
    "# init seq2seq model, the parameters needed are\n",
    "# embedding_size -> the size of the embedding for each word\n",
    "# hidden_size -> the number of hidden neurons per unit\n",
    "# voc_size -> the size of the vocabulary to embed each word\n",
    "# encoder = Encoder(embedding_size, hidden_size, vocabulary.__len__()).to(device)\n",
    "# decoder = Decoder(embedding_size, hidden_size, vocabulary.__len__()).to(device)\n",
    "\n",
    "encoder = EncoderAttention(embedding_size, hidden_size, vocabulary.__len__())\n",
    "attention = Attention(hidden_size)\n",
    "decoder = LuongAttentionDecoder(embedding_size, hidden_size, vocabulary.__len__(), attention=attention)\n",
    "\n",
    "model = ChatbotModel(encoder, decoder, vocabulary.__len__(), attention=True).to(device)\n",
    "\n",
    "print(model.state_dict())\n",
    "#init the optimizer\n",
    "optim = optim.Adam(model.parameters(), **optim_parameters)\n",
    "# init loss function\n",
    "# when computing the loss you want to ignore the '<PAD>' token.\n",
    "pad_index = vocabulary.word_to_idx['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "epoch_history = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    batch_history = []\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    for idx, X in enumerate(train_dataloader):\n",
    "        # transpose both input sentence and target sentence to access using the first dimension\n",
    "        # the the i-th word for each batch at each given time step t.\n",
    "        question = torch.transpose(X[0].to(device), 0, 1)\n",
    "        answer = torch.transpose(X[1].to(device), 0, 1)\n",
    "        # compute the output. Recall the output size should be (seq_len, batch_size, voc_size)\n",
    "        output = model(question, answer)\n",
    "        # don't consider the first element in all batches because it's the '<S>' token\n",
    "        output = output[1:].to(device)\n",
    "        answer = answer[1:].to(device)\n",
    "        # reshape both question and answer to the correct size for the loss function\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        answer = answer.reshape(-1)\n",
    "        # default previous weights values\n",
    "        optim.zero_grad()\n",
    "        # compute loss to backpropagate\n",
    "        loss = criterion(output, answer)\n",
    "        # backpropagate to compute gradients\n",
    "        loss.backward()\n",
    "        # clip gradients to avoid exploding values\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optim.step()\n",
    "        batch_history.append(loss.item())\n",
    "        # print current loss every 500 processed batches\n",
    "        if idx % 10 == 0:\n",
    "            print('BATCH [{}/{}], LOSS: {}'.format(idx, train_dataloader.__len__(), loss))\n",
    "    avg_loss = np.sum(batch_history) / train_dataloader.__len__()\n",
    "    return avg_loss\n",
    "\n",
    "def val_loop():\n",
    "    model.eval()\n",
    "    batch_history = []\n",
    "    avg_loss = 0\n",
    "    for idx, X in enumerate(val_dataloader):\n",
    "        # transpose both input sentence and target sentence to access using the first dimension\n",
    "        # the the i-th word for each batch at each given time step t.\n",
    "        question = torch.transpose(X[0].to(device), 0, 1)\n",
    "        answer = torch.transpose(X[1].to(device), 0, 1)\n",
    "        # compute the output. Recall the output size should be (seq_len, batch_size, voc_size)\n",
    "        output = model(question, answer)\n",
    "        # don't consider the first element in all batches because it's the '<S>' token\n",
    "        output = output[1:].to(device)\n",
    "        answer = answer[1:].to(device)\n",
    "        # reshape both question and answer to the correct size for the loss function\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        answer = answer.reshape(-1)\n",
    "        # compute the loss\n",
    "        loss = criterion(output, answer)\n",
    "\n",
    "        # keep track of the loss\n",
    "        batch_history.append(loss.item())\n",
    "        # print current loss every 500 processed batches\n",
    "        if idx % 10 == 0:\n",
    "            print('BATCH [{}/{}], LOSS: {}'.format(idx, val_dataloader.__len__(), loss))\n",
    "    avg_loss = np.sum(batch_history) / val_dataloader.__len__()\n",
    "    return avg_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "BATCH [0/1], LOSS: 10.821528434753418\n",
      "BATCH [0/1], LOSS: 10.83289623260498\n",
      "EPOCH [1/10] | Train Loss: 10.821528434753418 | Val. Loss: 10.83289623260498 | time: 0m 9s\n",
      "BATCH [0/1], LOSS: 10.821720123291016\n",
      "BATCH [0/1], LOSS: 10.827122688293457\n",
      "EPOCH [2/10] | Train Loss: 10.821720123291016 | Val. Loss: 10.827122688293457 | time: 0m 8s\n",
      "BATCH [0/1], LOSS: 10.820838928222656\n",
      "BATCH [0/1], LOSS: 10.825587272644043\n",
      "EPOCH [3/10] | Train Loss: 10.820838928222656 | Val. Loss: 10.825587272644043 | time: 0m 8s\n",
      "BATCH [0/1], LOSS: 10.815705299377441\n",
      "BATCH [0/1], LOSS: 10.833375930786133\n",
      "EPOCH [4/10] | Train Loss: 10.815705299377441 | Val. Loss: 10.833375930786133 | time: 0m 8s\n",
      "BATCH [0/1], LOSS: 10.821661949157715\n",
      "BATCH [0/1], LOSS: 10.822452545166016\n",
      "EPOCH [5/10] | Train Loss: 10.821661949157715 | Val. Loss: 10.822452545166016 | time: 0m 8s\n",
      "BATCH [0/1], LOSS: 10.824175834655762\n",
      "BATCH [0/1], LOSS: 10.820720672607422\n",
      "EPOCH [6/10] | Train Loss: 10.824175834655762 | Val. Loss: 10.820720672607422 | time: 0m 7s\n",
      "BATCH [0/1], LOSS: 10.81943130493164\n",
      "BATCH [0/1], LOSS: 10.820169448852539\n",
      "EPOCH [7/10] | Train Loss: 10.81943130493164 | Val. Loss: 10.820169448852539 | time: 0m 7s\n",
      "BATCH [0/1], LOSS: 10.821590423583984\n",
      "BATCH [0/1], LOSS: 10.820260047912598\n",
      "EPOCH [8/10] | Train Loss: 10.821590423583984 | Val. Loss: 10.820260047912598 | time: 0m 7s\n",
      "BATCH [0/1], LOSS: 10.819234848022461\n",
      "BATCH [0/1], LOSS: 10.829253196716309\n",
      "EPOCH [9/10] | Train Loss: 10.819234848022461 | Val. Loss: 10.829253196716309 | time: 0m 7s\n",
      "BATCH [0/1], LOSS: 10.814387321472168\n",
      "BATCH [0/1], LOSS: 10.823111534118652\n",
      "EPOCH [10/10] | Train Loss: 10.814387321472168 | Val. Loss: 10.823111534118652 | time: 0m 8s\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(os.curdir, 'saved_models/checkpoint.pth')\n",
    "path_saved_model = os.path.join(os.curdir, 'saved_models/trained_model.pth')\n",
    "# check if the model is already trained\n",
    "if os.path.exists(path_saved_model):\n",
    "    # load state_dict\n",
    "    model.load_state_dict(torch.load(path_saved_model))\n",
    "else:\n",
    "    # check if a training phase was already started\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # load trained values\n",
    "        loaded_checkpoint = torch.load(checkpoint_path)\n",
    "        # restore previous values\n",
    "        epoch = loaded_checkpoint['epoch']\n",
    "        model.load_state_dict(loaded_checkpoint['model_sd'])\n",
    "        optim.load_state_dict(loaded_checkpoint['optim_sd'])\n",
    "\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        # start counting epoch time\n",
    "        start_time = time.time()\n",
    "        # compute train loss\n",
    "        train_loss = train_loop()\n",
    "        # compute val loss\n",
    "        val_loss = val_loop()\n",
    "        # store train and val loss for later analysis\n",
    "        epoch_history.append((train_loss, val_loss))\n",
    "        # end of epoch\n",
    "        end_time = time.time()\n",
    "        # format elapsed time\n",
    "        elapsed_secs, elapsed_mins = format_time(start_time, end_time)\n",
    "        checkpoint = {'epoch': epoch,\n",
    "                      'optim_sd': optim.state_dict(),\n",
    "                      'model_sd':model.state_dict(),\n",
    "                      'train_loss': train_loss,\n",
    "                      'val_loss': val_loss\n",
    "                      }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(\"EPOCH [{}/{}] | Train Loss: {} | Val. Loss: {} | time: {}m {}s\".format(epoch+1,\n",
    "                                                                                           epochs,\n",
    "                                                                                           train_loss,\n",
    "                                                                                           val_loss,\n",
    "                                                                                           elapsed_mins,\n",
    "                                                                                           elapsed_secs))\n",
    "# save training model.\n",
    "print('Training completed.')\n",
    "torch.save(model.state_dict(), path_saved_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saboteur casbah hove fracturing menstrual destructs bullworker alliances filth myna eddies\n",
      "layout trimble sierras credence hitter robespierre zampf ofttimes mannered servant mannered\n",
      "layout trimble sierras credence hitter communistic korruptsky korruptsky anesthetic degre poked\n",
      "bridal sponsor auspices nineteenth layout layout trimble strapped eradicate degre poked\n"
     ]
    }
   ],
   "source": [
    "def pad_sequence(sequence, max_length):\n",
    "    pad_token_idx = vocabulary.word_to_idx['<PAD>']\n",
    "    while len(sequence) != max_length:\n",
    "        sequence.append(pad_token_idx)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def format_user_input(sequence, max_length=10):\n",
    "    # convert each word into index from the vocabulary\n",
    "    regex = re.compile('[^a-zA-Z0-9.!?]')\n",
    "    sequence = regex.sub(' ', sequence)\n",
    "    # remove extra space\n",
    "    sequence = re.sub(r\"([.!?])\", r\" \\1\", sequence)\n",
    "    # remove non-letter characters but keep regular punctuation\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sequence)\n",
    "    sequence = sequence.lower()\n",
    "    sequence = sequence.strip().split()\n",
    "    user_seq_indices = []\n",
    "    for word in sequence:\n",
    "        if word in vocabulary.word_to_idx.keys():\n",
    "            user_seq_indices.append(vocabulary.word_to_idx[word])\n",
    "        else:\n",
    "            user_seq_indices.append(vocabulary.word_to_idx['<UNK>'])\n",
    "\n",
    "    # pad or trim the sentence\n",
    "    if len(user_seq_indices) > max_length:  # trim\n",
    "        user_seq_indices = user_seq_indices[:max_length]\n",
    "    elif len(user_seq_indices) < max_length:  # pad\n",
    "        user_seq_indices = pad_sequence(user_seq_indices, max_length)\n",
    "    user_seq_indices.insert(0, vocabulary.word_to_idx['<S>'])\n",
    "    user_seq_indices.append(vocabulary.word_to_idx['</S>'])\n",
    "    user_seq_indices = torch.tensor(user_seq_indices).unsqueeze(1)\n",
    "    return user_seq_indices\n",
    "\n",
    "\n",
    "def convert_to_string(reply):\n",
    "    parsed_reply = []\n",
    "    for word_idx in reply:\n",
    "        # if the word is PAD or END of Sentence token, ignore it.\n",
    "        word_idx = word_idx.cpu().numpy()[0][0]\n",
    "        reply = vocabulary.vocab[word_idx]\n",
    "        if word_idx == '<PAD>' or word_idx == '</S>':\n",
    "            continue\n",
    "        else:\n",
    "            parsed_reply.append(reply)\n",
    "\n",
    "    return ' '.join(parsed_reply)\n",
    "\n",
    "\n",
    "def map_to_idx(sequence):\n",
    "    seq_idx = []\n",
    "    for word in sequence:\n",
    "        seq_idx.append(vocabulary.word_to_idx[word])\n",
    "    return seq_idx\n",
    "\n",
    "\n",
    "# evaluate the model to talk with it.\n",
    "def evaluate(seq, searcher):\n",
    "    # tensor should have shape [seq, 1]\n",
    "    seq = seq.to(device)\n",
    "    # feedforward to the searcher to get the list of most likely indices of words\n",
    "    bot_reply = searcher(seq)\n",
    "    # discard first element which is the start token\n",
    "    bot_reply = bot_reply[1:].to(device)\n",
    "    bot_reply = torch.topk(bot_reply, 1)\n",
    "    bot_reply = bot_reply.indices\n",
    "    # convert indices to words.\n",
    "    bot_reply = convert_to_string(bot_reply)\n",
    "    return bot_reply\n",
    "\n",
    "\n",
    "def run_bot(searcher, max_length=10):\n",
    "    user_input = \"\"\n",
    "    while(True):\n",
    "        try:\n",
    "            # ask the user for the input\n",
    "            user_input = input('> ')\n",
    "            # format the user input.\n",
    "            if user_input == 'quit':\n",
    "                break\n",
    "            user_input_idx = format_user_input(user_input, max_length)\n",
    "            # run the evaluate function to get bot's reply\n",
    "            reply = evaluate(user_input_idx, searcher)\n",
    "            print(reply)\n",
    "        except KeyError:\n",
    "            print('Error: While parsing the input sentence...')\n",
    "\n",
    "# run bot and begin dialog\n",
    "searcher = GreedySearch(encoder, decoder, vocabulary, attention=True)\n",
    "run_bot(searcher)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}